{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WZp53HmMP3F2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('default')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TovFkDTgE_d6",
        "outputId": "2b93f94e-3dd4-4d59-b36e-3f23c50b49a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-12-09 10:33:08--  http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip [following]\n",
            "--2025-12-09 10:33:09--  https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 789539 (771K) [application/zip]\n",
            "Saving to: ‘trainDevTestTrees_PTB.zip’\n",
            "\n",
            "trainDevTestTrees_P 100%[===================>] 771.03K  1.03MB/s    in 0.7s    \n",
            "\n",
            "2025-12-09 10:33:09 (1.03 MB/s) - ‘trainDevTestTrees_PTB.zip’ saved [789539/789539]\n",
            "\n",
            "Archive:  trainDevTestTrees_PTB.zip\n",
            "   creating: trees/\n",
            "  inflating: trees/dev.txt           \n",
            "  inflating: trees/test.txt          \n",
            "  inflating: trees/train.txt         \n"
          ]
        }
      ],
      "source": [
        "# !wget http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
        "# !unzip trainDevTestTrees_PTB.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0IpAphkBO5eW"
      },
      "outputs": [],
      "source": [
        "# this function reads in a textfile and fixes an issue with \"\\\\\"\n",
        "def filereader(path):\n",
        "  with open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "      yield line.strip().replace(\"\\\\\",\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP_jpquiprH8"
      },
      "source": [
        "Let's look at a data point. It is a **flattened binary tree**, with sentiment scores at every node, and words as the leaves (or *terminal nodes*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylkIopm0QJML",
        "outputId": "0030096a-55b2-47a9-abf8-46bdf1968dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\n"
          ]
        }
      ],
      "source": [
        "s = next(filereader(\"trees/dev.txt\"))\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_U7HTFwdrWt",
        "outputId": "3a8543ea-c3b9-48df-adc6-4493d9b8a305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              3                                                                     \n",
            "  ____________|____________________                                                  \n",
            " |                                 4                                                \n",
            " |        _________________________|______________________________________________   \n",
            " |       4                                                                        | \n",
            " |    ___|______________                                                          |  \n",
            " |   |                  4                                                         | \n",
            " |   |         _________|__________                                               |  \n",
            " |   |        |                    3                                              | \n",
            " |   |        |               _____|______________________                        |  \n",
            " |   |        |              |                            4                       | \n",
            " |   |        |              |            ________________|_______                |  \n",
            " |   |        |              |           |                        2               | \n",
            " |   |        |              |           |                 _______|___            |  \n",
            " |   |        3              |           |                |           2           | \n",
            " |   |    ____|_____         |           |                |        ___|_____      |  \n",
            " |   |   |          4        |           3                |       2         |     | \n",
            " |   |   |     _____|___     |      _____|_______         |    ___|___      |     |  \n",
            " 2   2   2    3         2    2     3             2        2   2       2     2     2 \n",
            " |   |   |    |         |    |     |             |        |   |       |     |     |  \n",
            " It  's  a  lovely     film with lovely     performances  by Buy     and Accorsi  . \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/m5/wwbxgfjj3g7bn2wdsmlw55j80000gn/T/ipykernel_73089/348436308.py:5: DeprecationWarning: \n",
            "    Class TreePrettyPrinter has been deprecated.  Import\n",
            "    `TreePrettyPrinter` using `from nltk.tree import\n",
            "    TreePrettyPrinter` instead.\n",
            "  print(TreePrettyPrinter(tree))\n"
          ]
        }
      ],
      "source": [
        "# We can use NLTK to better visualise the tree structure of the sentence\n",
        "from nltk import Tree\n",
        "from nltk.treeprettyprinter import TreePrettyPrinter\n",
        "tree = Tree.fromstring(s)\n",
        "print(TreePrettyPrinter(tree))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekAWKsji9t93"
      },
      "source": [
        "The sentiment scores range from 0 (very negative) to 5 (very positive). Again, as you can see, every node in the tree is labeled with a sentiment score. For now, we will only use the score at the **root node**, i.e., the sentiment score for the complete sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKynLm0xPKr2",
        "outputId": "81cdc220-de74-4e09-a300-da619c1be3f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
            "13\n"
          ]
        }
      ],
      "source": [
        "# Let's first make a function that extracts the tokens (the leaves).\n",
        "\n",
        "def tokens_from_treestring(s):\n",
        "  \"\"\"extract the tokens from a sentiment tree\"\"\"\n",
        "  return re.sub(r\"\\([0-9] |\\)\", \"\", s).split()\n",
        "\n",
        "# let's try it on our example tree\n",
        "tokens = tokens_from_treestring(s)\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8vFkeqN-NLP"
      },
      "source": [
        "> *Warning: you could also parse a treestring using NLTK and ask it to return the leaves, but there seems to be an issue with NLTK not always correctly parsing the input, so do not rely on it.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Akr9K_Mv4dym",
        "outputId": "a7e46736-19e0-4ed4-f2ca-4eb08795fe8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:9: SyntaxWarning: invalid escape sequence '\\('\n",
            "<>:10: SyntaxWarning: invalid escape sequence '\\)'\n",
            "<>:11: SyntaxWarning: invalid escape sequence '\\('\n",
            "<>:12: SyntaxWarning: invalid escape sequence '\\('\n",
            "<>:13: SyntaxWarning: invalid escape sequence '\\)'\n",
            "<>:9: SyntaxWarning: invalid escape sequence '\\('\n",
            "<>:10: SyntaxWarning: invalid escape sequence '\\)'\n",
            "<>:11: SyntaxWarning: invalid escape sequence '\\('\n",
            "<>:12: SyntaxWarning: invalid escape sequence '\\('\n",
            "<>:13: SyntaxWarning: invalid escape sequence '\\)'\n",
            "/var/folders/m5/wwbxgfjj3g7bn2wdsmlw55j80000gn/T/ipykernel_73089/1625531552.py:9: SyntaxWarning: invalid escape sequence '\\('\n",
            "  s = re.sub(\"\\([0-5] ([^)]+)\\)\", \"0\", s)\n",
            "/var/folders/m5/wwbxgfjj3g7bn2wdsmlw55j80000gn/T/ipykernel_73089/1625531552.py:10: SyntaxWarning: invalid escape sequence '\\)'\n",
            "  s = re.sub(\"\\)\", \" )\", s)\n",
            "/var/folders/m5/wwbxgfjj3g7bn2wdsmlw55j80000gn/T/ipykernel_73089/1625531552.py:11: SyntaxWarning: invalid escape sequence '\\('\n",
            "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
            "/var/folders/m5/wwbxgfjj3g7bn2wdsmlw55j80000gn/T/ipykernel_73089/1625531552.py:12: SyntaxWarning: invalid escape sequence '\\('\n",
            "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
            "/var/folders/m5/wwbxgfjj3g7bn2wdsmlw55j80000gn/T/ipykernel_73089/1625531552.py:13: SyntaxWarning: invalid escape sequence '\\)'\n",
            "  s = re.sub(\"\\)\", \"1\", s)\n"
          ]
        }
      ],
      "source": [
        "# We will also need the following function, but you can ignore this for now.\n",
        "# It is explained later on.\n",
        "\n",
        "SHIFT = 0\n",
        "REDUCE = 1\n",
        "\n",
        "\n",
        "def transitions_from_treestring(s):\n",
        "  s = re.sub(\"\\([0-5] ([^)]+)\\)\", \"0\", s)\n",
        "  s = re.sub(\"\\)\", \" )\", s)\n",
        "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
        "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
        "  s = re.sub(\"\\)\", \"1\", s)\n",
        "  return list(map(int, s.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNtPdlwPgRat",
        "outputId": "26b5dee2-b134-4ef8-8d79-35c842d9bf07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trees/train.txt  8544\n",
            "trees/dev.txt    1101\n",
            "trees/test.txt   2210\n"
          ]
        }
      ],
      "source": [
        "# Now let's first see how large our data sets are.\n",
        "for path in (\"trees/train.txt\", \"trees/dev.txt\", \"trees/test.txt\"):\n",
        "  print(\"{:16s} {:4d}\".format(path, sum(1 for _ in filereader(path))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HexlSqTR_UrY"
      },
      "source": [
        "You can see that the number of sentences is not very large. That's probably because the data set required so much manual annotation. However, it is large enough to train a neural network on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfRjelOcsXuC"
      },
      "source": [
        "It will be useful to store each data example in an `Example` object,\n",
        "containing everything that we may need for each data point.\n",
        "It will contain the tokens, the tree, the top-level sentiment label, and\n",
        "the transitions (explained later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I07Hb_-q8wg",
        "outputId": "ca06edb5-547e-45ea-f3d1-1b9920b6be82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train 8544\n",
            "dev 1101\n",
            "test 2210\n"
          ]
        }
      ],
      "source": [
        "from collections import namedtuple\n",
        "from nltk import Tree\n",
        "\n",
        "# A simple way to define a class is using namedtuple.\n",
        "Example = namedtuple(\"Example\", [\"tokens\", \"tree\", \"label\", \"transitions\"])\n",
        "\n",
        "\n",
        "def examplereader(path, lower=False):\n",
        "  \"\"\"Returns all examples in a file one by one.\"\"\"\n",
        "  for line in filereader(path):\n",
        "    line = line.lower() if lower else line\n",
        "    tokens = tokens_from_treestring(line)\n",
        "    tree = Tree.fromstring(line)  # use NLTK's Tree\n",
        "    label = int(line[1])\n",
        "    trans = transitions_from_treestring(line)\n",
        "    yield Example(tokens=tokens, tree=tree, label=label, transitions=trans)\n",
        "\n",
        "\n",
        "# Let's load the data into memory.\n",
        "LOWER = False  # we will keep the original casing\n",
        "train_data = list(examplereader(\"trees/train.txt\", lower=LOWER))\n",
        "dev_data = list(examplereader(\"trees/dev.txt\", lower=LOWER))\n",
        "test_data = list(examplereader(\"trees/test.txt\", lower=LOWER))\n",
        "\n",
        "print(\"train\", len(train_data))\n",
        "print(\"dev\", len(dev_data))\n",
        "print(\"test\", len(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KM0bDyeVZtP"
      },
      "source": [
        "Let's check out an `Example` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VvNgKx7usRSt"
      },
      "outputs": [],
      "source": [
        "# Here we first define a class that can map a word to an ID (w2i)\n",
        "# and back (i2w).\n",
        "\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "\n",
        "\n",
        "class OrderedCounter(Counter, OrderedDict):\n",
        "  \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
        "  def __repr__(self):\n",
        "    return '%s(%r)' % (self.__class__.__name__,\n",
        "                      OrderedDict(self))\n",
        "  def __reduce__(self):\n",
        "    return self.__class__, (OrderedDict(self),)\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "  \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.freqs = OrderedCounter()\n",
        "    self.w2i = {}\n",
        "    self.i2w = []\n",
        "\n",
        "  def count_token(self, t):\n",
        "    self.freqs[t] += 1\n",
        "\n",
        "  def add_token(self, t):\n",
        "    self.w2i[t] = len(self.w2i)\n",
        "    self.i2w.append(t)\n",
        "\n",
        "  def build(self, min_freq=0):\n",
        "    '''\n",
        "    min_freq: minimum number of occurrences for a word to be included\n",
        "              in the vocabulary\n",
        "    '''\n",
        "    self.add_token(\"<unk>\")  # reserve 0 for <unk> (unknown words)\n",
        "    self.add_token(\"<pad>\")  # reserve 1 for <pad> (discussed later)\n",
        "\n",
        "    tok_freq = list(self.freqs.items())\n",
        "    tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
        "    for tok, freq in tok_freq:\n",
        "      if freq >= min_freq:\n",
        "        self.add_token(tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOvkH_llVsoW"
      },
      "source": [
        "The vocabulary has by default an `<unk>` token and a `<pad>` token. The `<unk>` token is reserved for all words which do not appear in the training data (and for which, therefore, we cannot learn word representations). The function of the `<pad>` token will be explained later.\n",
        "\n",
        "\n",
        "Let's build the vocabulary!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwGQgQQBNUSq",
        "outputId": "f819580c-b6f6-4a50-9f29-02ff5e253938"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 18280\n"
          ]
        }
      ],
      "source": [
        "# This process should be deterministic and should have the same result\n",
        "# if run multiple times on the same data set.\n",
        "\n",
        "v = Vocabulary()\n",
        "for data_set in (train_data,):\n",
        "  for ex in data_set:\n",
        "    for token in ex.tokens:\n",
        "      v.count_token(token)\n",
        "\n",
        "v.build()\n",
        "print(\"Vocabulary size:\", len(v.w2i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UNIedPrPdCw"
      },
      "source": [
        "Let's have a closer look at the properties of our vocabulary. Having a good idea of what it is like can facilitate data analysis and debugging later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmTC-rvQelpl",
        "outputId": "867f3cf6-c0bf-4cbb-ad64-a4cf79a430de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['very negative', 'negative', 'neutral', 'positive', 'very positive']\n",
            "very positive\n"
          ]
        }
      ],
      "source": [
        "# Now let's map the sentiment labels 0-4 to a more readable form\n",
        "i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
        "print(i2t)\n",
        "print(i2t[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7UI26DP2dr2",
        "outputId": "6a28c7ef-db78-44e0-c810-e2e589fddee3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OrderedDict({'very negative': 0, 'negative': 1, 'neutral': 2, 'positive': 3, 'very positive': 4})\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "# And let's also create the opposite mapping.\n",
        "# We won't use a Vocabulary for this (although we could), since the labels\n",
        "# are already numeric.\n",
        "t2i = OrderedDict({p : i for p, i in zip(i2t, range(len(i2t)))})\n",
        "print(t2i)\n",
        "print(t2i['very positive'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0067ax54-rd"
      },
      "source": [
        "## PyTorch\n",
        "\n",
        "We are going to need PyTorch and Google Colab does not have it installed by default. Run the cell below to install it.\n",
        "\n",
        "*For installing PyTorch in your own computer, follow the instructions on [pytorch.org](pytorch.org) instead. This is for Google Colab only.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKQMGtkR5KWr",
        "outputId": "7e3c05cb-a41a-4ae6-c975-a50da80cde4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using torch 2.6.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"Using torch\", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mnvPcd_E1xH8"
      },
      "outputs": [],
      "source": [
        "# Let's also import torch.nn, a PyTorch package that\n",
        "# makes building neural networks more convenient.\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYt8uTyGCKc7",
        "outputId": "f8f50818-9ea5-453f-f4ac-82decdebea1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# PyTorch can run on CPU or on Nvidia GPU (video card) using CUDA\n",
        "# This cell selects the GPU if one is available.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2d1VMOOYx1Bw"
      },
      "outputs": [],
      "source": [
        "# Seed manually to make runs reproducible\n",
        "# You need to set this again if you do multiple runs of the same model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# When running on the CuDNN backend two further options must be set for reproducibility\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWBTzkuE3CtZ"
      },
      "source": [
        "# BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Fhvk5HenAroT"
      },
      "outputs": [],
      "source": [
        "# Here we print each parameter name, shape, and if it is trainable.\n",
        "def print_parameters(model):\n",
        "  total = 0\n",
        "  for name, p in model.named_parameters():\n",
        "    total += np.prod(p.shape)\n",
        "    print(\"{:24s} {:12s} requires_grad={}\".format(name, str(list(p.shape)), p.requires_grad))\n",
        "  print(\"\\nTotal number of parameters: {}\\n\".format(total))\n",
        "\n",
        "\n",
        "#print_parameters(bow_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YWeGTC_OGReV"
      },
      "outputs": [],
      "source": [
        "def prepare_example(example, vocab):\n",
        "  \"\"\"\n",
        "  Map tokens to their IDs for a single example\n",
        "  \"\"\"\n",
        "\n",
        "  # vocab returns 0 if the word is not there (i2w[0] = <unk>)\n",
        "  x = [vocab.w2i.get(t, 0) for t in example.tokens]\n",
        "\n",
        "  x = torch.LongTensor([x])\n",
        "  x = x.to(device)\n",
        "\n",
        "  y = torch.LongTensor([example.label])\n",
        "  y = y.to(device)\n",
        "\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yGmQLcVYKZsh"
      },
      "outputs": [],
      "source": [
        "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
        "  \"\"\"Accuracy of a model on given data set.\"\"\"\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model.eval()  # disable dropout (explained later)\n",
        "\n",
        "  for example in data:\n",
        "\n",
        "    # convert the example input and label to PyTorch tensors\n",
        "    x, target = prep_fn(example, model.vocab)\n",
        "\n",
        "    # forward pass without backpropagation (no_grad)\n",
        "    # get the output from the neural network for input x\n",
        "    with torch.no_grad():\n",
        "      logits = model(x)\n",
        "\n",
        "    # get the prediction\n",
        "    prediction = logits.argmax(dim=-1)\n",
        "\n",
        "    # add the number of correct predictions to the total correct\n",
        "    correct += (prediction == target).sum().item()\n",
        "    total += 1\n",
        "\n",
        "  return correct, total, correct / float(total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KlIGFXllWWm"
      },
      "source": [
        "We are using accuracy as a handy evaluation metric. Please consider using [alternative metrics](https://scikit-learn.org/stable/modules/classes.html#classification-metrics) for your experiments if that makes more theoretical sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIk6OtSdzGRP"
      },
      "source": [
        "#### Example feed\n",
        "For stochastic gradient descent (SGD) we will need a random training example for every update.\n",
        "We implement this by shuffling the training data and returning examples one by one using `yield`.\n",
        "\n",
        "Shuffling is optional so that we get to use this function to get validation and test examples, too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dxDFOZLfCXvJ"
      },
      "outputs": [],
      "source": [
        "def get_examples(data, shuffle=True, **kwargs):\n",
        "  \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
        "  if shuffle:\n",
        "    print(\"Shuffling training data\")\n",
        "    random.shuffle(data)  # shuffle training data each epoch\n",
        "  for example in data:\n",
        "    yield example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g09SM8yb2cjx"
      },
      "source": [
        "#### Exercise: Training function\n",
        "\n",
        "Your task is now to complete the training loop below.\n",
        "Before you do so, please read the section about optimisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KhQigDrQ--YU"
      },
      "outputs": [],
      "source": [
        "from torch import optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ktFnKBux25lD"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, num_iterations=10000,\n",
        "                print_every=1000, eval_every=1000,\n",
        "                batch_fn=get_examples,\n",
        "                prep_fn=prepare_example,\n",
        "                eval_fn=simple_evaluate,\n",
        "                batch_size=1, eval_batch_size=None):\n",
        "  \"\"\"Train a model.\"\"\"\n",
        "  iter_i = 0\n",
        "  train_loss = 0.\n",
        "  print_num = 0\n",
        "  start = time.time()\n",
        "  criterion = nn.CrossEntropyLoss() # loss function\n",
        "  best_eval = 0.\n",
        "  best_iter = 0\n",
        "\n",
        "  # store train loss and validation accuracy during training\n",
        "  # so we can plot them afterwards\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "\n",
        "  if eval_batch_size is None:\n",
        "    eval_batch_size = batch_size\n",
        "\n",
        "  while True:  # when we run out of examples, shuffle and continue\n",
        "    for batch in batch_fn(train_data, batch_size=batch_size):\n",
        "\n",
        "      # forward pass\n",
        "      model.train()\n",
        "      x, targets = prep_fn(batch, model.vocab)\n",
        "      logits = model(x)\n",
        "\n",
        "      B = targets.size(0)  # later we will use B examples per update\n",
        "\n",
        "      # compute cross-entropy loss (our criterion)\n",
        "      # note that the cross entropy loss function computes the softmax for us\n",
        "      loss = criterion(logits.view([B, -1]), targets.view(-1))\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # backward pass (tip: check the Introduction to PyTorch notebook)\n",
        "\n",
        "      # ========== MY CODE ==============\n",
        "      # erase previous gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # compute gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # update weights - take a small step in the opposite dir of the gradient\n",
        "      optimizer.step()\n",
        "\n",
        "      print_num += 1\n",
        "      iter_i += 1\n",
        "\n",
        "      # print info\n",
        "      if iter_i % print_every == 0:\n",
        "        print(\"Iter %r: loss=%.4f, time=%.2fs\" %\n",
        "              (iter_i, train_loss, time.time()-start))\n",
        "        losses.append(train_loss)\n",
        "        print_num = 0\n",
        "        train_loss = 0.\n",
        "\n",
        "      # evaluate\n",
        "      if iter_i % eval_every == 0:\n",
        "        _, _, accuracy = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
        "                                 batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "        accuracies.append(accuracy)\n",
        "        print(\"iter %r: dev acc=%.4f\" % (iter_i, accuracy))\n",
        "\n",
        "        # save best model parameters\n",
        "        if accuracy > best_eval:\n",
        "          print(\"new highscore\")\n",
        "          best_eval = accuracy\n",
        "          best_iter = iter_i\n",
        "          path = \"{}.pt\".format(model.__class__.__name__)\n",
        "          ckpt = {\n",
        "              \"state_dict\": model.state_dict(),\n",
        "              \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "              \"best_eval\": best_eval,\n",
        "              \"best_iter\": best_iter\n",
        "          }\n",
        "          torch.save(ckpt, path)\n",
        "\n",
        "      # done training\n",
        "      if iter_i == num_iterations:\n",
        "        print(\"Done training\")\n",
        "\n",
        "        # evaluate on train, dev, and test with best model\n",
        "        print(\"Loading best model\")\n",
        "        path = \"{}.pt\".format(model.__class__.__name__)\n",
        "        ckpt = torch.load(path)\n",
        "        model.load_state_dict(ckpt[\"state_dict\"])\n",
        "\n",
        "        _, _, train_acc = eval_fn(\n",
        "            model, train_data, batch_size=eval_batch_size,\n",
        "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "        _, _, dev_acc = eval_fn(\n",
        "            model, dev_data, batch_size=eval_batch_size,\n",
        "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "        _, _, test_acc = eval_fn(\n",
        "            model, test_data, batch_size=eval_batch_size,\n",
        "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "\n",
        "        print(\"best model iter {:d}: \"\n",
        "              \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
        "                  best_iter, train_acc, dev_acc, test_acc))\n",
        "\n",
        "        return losses, accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "unzLbNnQnrY3"
      },
      "outputs": [],
      "source": [
        "def train_model_w_subtree(model, optimizer, num_iterations=10000,\n",
        "                          print_every=1000, eval_every=1000,\n",
        "                          batch_fn=get_examples,\n",
        "                          prep_fn=prepare_example,\n",
        "                          eval_fn=simple_evaluate,\n",
        "                          batch_size=1, eval_batch_size=None):\n",
        "    \"\"\"Train a model with subtree supervision.\"\"\"\n",
        "    iter_i = 0\n",
        "    train_loss = 0.\n",
        "    start = time.time()\n",
        "    criterion = nn.CrossEntropyLoss()  # loss function\n",
        "    best_eval = 0.\n",
        "    best_iter = 0\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    if eval_batch_size is None:\n",
        "        eval_batch_size = batch_size\n",
        "\n",
        "    while True:\n",
        "        for batch in batch_fn(train_data, batch_size=batch_size):\n",
        "            model.train()\n",
        "\n",
        "            # --- prepare inputs ---\n",
        "            # prep_fn should now return subtree labels as well\n",
        "            (x, transitions), targets, subtree_labels = prep_fn(batch, model.vocab)\n",
        "            root_logits, subtree_logits = model((x, transitions))\n",
        "\n",
        "            B = targets.size(0)\n",
        "\n",
        "            # --- compute root loss ---\n",
        "            loss_root = criterion(root_logits, targets)\n",
        "\n",
        "            # --- compute subtree loss ---\n",
        "            flat_subtree_logits = []\n",
        "            flat_subtree_labels = []\n",
        "\n",
        "            for b, sent_nodes in enumerate(subtree_logits):\n",
        "                for i, node_logit in enumerate(sent_nodes):\n",
        "                    flat_subtree_logits.append(node_logit)\n",
        "                    flat_subtree_labels.append(subtree_labels[b][i])\n",
        "\n",
        "            # flat_subtree_logits = torch.stack(flat_subtree_logits, dim=0)\n",
        "            # flat_subtree_labels = [int(lbl) for lbl in flat_subtree_labels]\n",
        "            # flat_subtree_labels = torch.tensor(flat_subtree_labels, device=flat_subtree_logits.device)\n",
        "\n",
        "            # flat_subtree_logits: [total_nodes, num_classes]\n",
        "            flat_subtree_logits = torch.stack(flat_subtree_logits, dim=0)\n",
        "            flat_subtree_logits = flat_subtree_logits.squeeze(1)  # [N, C]\n",
        "\n",
        "            # flat_subtree_labels: [total_nodes], as integers\n",
        "            flat_subtree_labels = torch.tensor([int(lbl) for lbl in flat_subtree_labels],\n",
        "                                              dtype=torch.long, device=flat_subtree_logits.device)\n",
        "\n",
        "            #print(f\"flat_subtree_logits : {flat_subtree_logits.size()}\")\n",
        "            #print(f\"flat_subtree_labels : {flat_subtree_labels.size()}\")\n",
        "            loss_subtree = criterion(flat_subtree_logits, flat_subtree_labels)\n",
        "\n",
        "            # combine losses (you can weight them if desired)\n",
        "            loss = loss_root + loss_subtree\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # --- backward pass ---\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            iter_i += 1\n",
        "\n",
        "            # --- printing and evaluation ---\n",
        "            if iter_i % print_every == 0:\n",
        "                print(f\"Iter {iter_i}: loss={train_loss:.4f}, time={time.time()-start:.2f}s\")\n",
        "                losses.append(train_loss)\n",
        "                train_loss = 0.\n",
        "\n",
        "            if iter_i % eval_every == 0:\n",
        "                _, _, accuracy = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
        "                                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "                accuracies.append(accuracy)\n",
        "                print(f\"Iter {iter_i}: dev acc={accuracy:.4f}\")\n",
        "\n",
        "                if accuracy > best_eval:\n",
        "                    print(\"New highscore!\")\n",
        "                    best_eval = accuracy\n",
        "                    best_iter = iter_i\n",
        "                    path = f\"{model.__class__.__name__}.pt\"\n",
        "                    ckpt = {\n",
        "                        \"state_dict\": model.state_dict(),\n",
        "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                        \"best_eval\": best_eval,\n",
        "                        \"best_iter\": best_iter\n",
        "                    }\n",
        "                    torch.save(ckpt, path)\n",
        "\n",
        "            if iter_i == num_iterations:\n",
        "                print(\"Done training\")\n",
        "\n",
        "                # load best model for final evaluation\n",
        "                path = f\"{model.__class__.__name__}.pt\"\n",
        "                ckpt = torch.load(path)\n",
        "                model.load_state_dict(ckpt[\"state_dict\"])\n",
        "\n",
        "                _, _, train_acc = eval_fn(model, train_data, batch_size=eval_batch_size,\n",
        "                                          batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "                _, _, dev_acc = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
        "                                        batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "                _, _, test_acc = eval_fn(model, test_data, batch_size=eval_batch_size,\n",
        "                                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "\n",
        "                print(f\"Best model iter {best_iter}: \"\n",
        "                      f\"train acc={train_acc:.4f}, dev acc={dev_acc:.4f}, test acc={test_acc:.4f}\")\n",
        "                return losses, accuracies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEPsLvI-3D5b"
      },
      "source": [
        "### Training the BOW model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQZ5flHwiiHY"
      },
      "source": [
        "# Pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NX35vecmHy6"
      },
      "source": [
        "The Stanford Sentiment Treebank is a rather small data set, since it required fine-grained manual annotatation. This makes it difficult for the Deep CBOW model to learn good word embeddings, i.e. to learn good word representations for the words in our vocabulary.\n",
        "In fact, the only error signal that the network receives is from predicting the sentiment of entire sentences!\n",
        "\n",
        "To start off with better word representations, we can download **pre-trained word embeddings**.\n",
        "You can choose which pre-trained word embeddings to use:\n",
        "\n",
        "- **GloVe**. The \"original\" Stanford Sentiment classification [paper](http://aclweb.org/anthology/P/P15/P15-1150.pdf) used Glove embeddings, which are just another method (like *word2vec*) to get word embeddings from unannotated text. Glove is described in the following paper which you should cite if you use them:\n",
        "> Jeffrey Pennington, Richard Socher, and Christopher Manning. [\"Glove: Global vectors for word representation.\"](https://nlp.stanford.edu/pubs/glove.pdf) EMNLP 2014.\n",
        "\n",
        "- **Word2Vec**. This is the method that you learned about in class, described in:\n",
        "> Mikolov, Tomas, et al. [\"Distributed representations of words and phrases and their compositionality.\"](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) Advances in neural information processing systems. 2013.\n",
        "\n",
        "Using these pre-trained word embeddings, we can initialize our word embedding lookup table and start form a point where similar words are already close to one another in the distributional semantic space.\n",
        "\n",
        "You can choose to keep the word embeddings **fixed** or to train them further, specialising them to the task at hand.\n",
        "We will keep them fixed for now.\n",
        "\n",
        "For the purposes of this lab, it is enough if you understand how word2vec works (whichever vectors you use), but if you are interested, we encourage you to also check out the GloVe paper.\n",
        "\n",
        "You can either download the word2vec vectors, or the Glove vectors.\n",
        "If you want to compare your results to the Stanford paper later on, then you should use Glove.\n",
        "**At the end of this lab you have the option to compare which vectors give you the best performance. For now, simply choose one of them and continue with that.**\n",
        "\n",
        "[**OPTIONAL in case you don't want to mount Google Drive:** instead of running all the 5 boxes below, you can 1) download the GloVe and word2vec in your local machine, 2) upload them on your Drive folder (\"My Drive\"). Then, uncomment the first 2 lines in box 6 before writing your code!]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGYr02WWO993"
      },
      "outputs": [],
      "source": [
        "# This downloads the Glove 840B 300d embeddings.\n",
        "# The original file is at http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "# Since that file is 2GB, we provide you with a *filtered version*\n",
        "# which contains all the words you need for this data set.\n",
        "\n",
        "# You only need to do this once.\n",
        "# Please comment this cell out after downloading.\n",
        "\n",
        "#!wget https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NLsgFGiTjmI"
      },
      "outputs": [],
      "source": [
        "# This downloads the word2vec 300D Google News vectors\n",
        "# The file has been truncated to only contain words that appear in our data set.\n",
        "# You can find the original file here: https://code.google.com/archive/p/word2vec/\n",
        "\n",
        "# You only need to do this once.\n",
        "# Please comment this out after downloading.\n",
        "#!wget https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXBITzPRQUQb",
        "outputId": "75a3ce50-08ed-43cf-d2e6-802d28c5f65a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (to save the downloaded files)\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFvzPuiKSCbl"
      },
      "outputs": [],
      "source": [
        "# Copy word vectors *to* Google Drive\n",
        "\n",
        "# You only need to do this once.\n",
        "# Please comment this out after running it.\n",
        "# !cp \"glove.840B.300d.sst.txt\" \"/gdrive/My Drive/\"\n",
        "# !cp \"googlenews.word2vec.300d.txt\" \"/gdrive/My Drive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUMH0bM6BuY9"
      },
      "outputs": [],
      "source": [
        "# If you copied the word vectors to your Drive before,\n",
        "# here is where you copy them back to the Colab notebook.\n",
        "\n",
        "# Copy Glove vectors *from* Google Drive\n",
        "# !cp \"/gdrive/My Drive/glove.840B.300d.sst.txt\" .\n",
        "# !cp \"/gdrive/My Drive/googlenews.word2vec.300d.txt\" ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcpkoh6PIjfe"
      },
      "outputs": [],
      "source": [
        "# Uncomment these 2 lines below if went for the OPTIONAL method described above\n",
        "# !cp \"glove.840B.300d.sst.txt\" \"./\"\n",
        "# !cp \"googlenews.word2vec.300d.txt\" \"./\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File 'googlenews.word2vec.300d.txt' already exists. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Download the Word2Vec file (Replaces !wget) ---\n",
        "# Check if the file already exists to avoid re-downloading\n",
        "url = \"https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt\"\n",
        "filename = \"googlenews.word2vec.300d.txt\"\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    print(f\"Downloading {filename}...\")\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "        print(\"Download complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during download: {e}\")\n",
        "else:\n",
        "    print(f\"File '{filename}' already exists. Skipping download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX2GJVHILM8n"
      },
      "source": [
        "At this point you have the pre-trained word embedding files, but what do they look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChsChH14Ruxn",
        "outputId": "5c2d24aa-55c9-429e-cb65-3641f64651ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing file and printing first 4 lines:\n",
            "Line 1: in 0.0703125 0.08691406 0.087890625 0.0625 0.06933594 -0.10888672 -0.08154297 -0.15429688 0.020751953 0.13183594 -0.11376953 -0.037353516 0.06933594 0.078125 -0.103027344 -0.09765625 0.044189453 0.10253906 -0.060791016 -0.036132812 -0.045410156 0.04736328 -0.12060547 -0.063964844 0.0022583008 0.037109375 -0.0029144287 0.11767578 0.061767578 0.063964844 0.08105469 -0.068847656 -0.021362305 0.05517578 -0.08544922 0.068847656 -0.12792969 -0.033203125 0.09863281 0.17578125 0.110839844 -0.03466797 -0.04711914 -0.008483887 0.035888672 0.103027344 0.026977539 -0.028686523 -0.005126953 0.10644531 0.059814453 0.09423828 0.033691406 -0.02709961 -0.09423828 0.0010299683 -0.048339844 0.034423828 0.08105469 -0.11328125 -0.08886719 0.035888672 -0.14550781 -0.24414062 -0.061523438 0.052978516 0.056884766 0.1796875 0.061035156 0.08691406 0.12402344 -0.040283203 0.022583008 0.17773438 -0.029663086 -0.029663086 0.1171875 0.03112793 -0.096191406 0.06640625 0.004699707 -0.080078125 0.06298828 -0.020629883 -0.0546875 -0.13574219 -0.06347656 0.083496094 -0.063964844 0.021484375 0.07714844 -0.037109375 -0.033691406 -0.18359375 -0.072753906 0.01586914 0.09326172 -0.061523438 -0.014221191 -0.0034484863 0.011108398 -0.15820312 -0.017089844 0.0061950684 -0.008728027 -0.080566406 -0.015258789 -0.087890625 0.003479004 -0.016113281 -0.012329102 0.09765625 -0.13964844 -0.0859375 -0.026855469 0.053955078 0.1328125 0.11279297 0.12109375 0.08544922 -0.0071105957 0.044677734 -0.14550781 -0.0032043457 -0.11767578 -0.06542969 0.07128906 -0.09423828 -0.030273438 0.12011719 0.080078125 -0.09472656 -0.16210938 -0.07763672 0.021240234 -0.08154297 0.0039367676 -0.15722656 -0.09814453 0.039794922 0.03930664 -0.009094238 0.103027344 0.067871094 -0.04272461 0.06347656 -0.049072266 0.020874023 -0.16699219 0.09326172 0.09375 0.006866455 0.053710938 0.052490234 -0.024414062 -0.032470703 -0.061523438 -0.005554199 0.096191406 0.037841797 0.012207031 -0.043945312 -0.0074768066 0.10546875 0.020385742 0.14550781 0.08203125 0.0057678223 0.0045776367 -0.09277344 -0.13867188 -0.057373047 -0.051513672 -0.13085938 -0.13964844 -0.020507812 -0.02709961 0.032714844 0.10498047 -0.0023345947 -0.022583008 0.00050354004 -0.110839844 0.08496094 -0.12988281 -0.017456055 -0.00035858154 0.107910156 0.08886719 0.044677734 0.025146484 0.023803711 0.08105469 0.02368164 -0.10986328 0.0053710938 -0.017700195 -0.033935547 -0.032958984 -0.1640625 0.095703125 -0.018310547 0.0053100586 -0.034423828 -0.044189453 -0.06640625 -0.017944336 -0.029663086 -0.007598877 -0.05126953 -0.05419922 0.08935547 -0.071777344 0.015258789 -0.08251953 -0.03173828 0.03564453 -0.021240234 -0.059326172 -0.013061523 0.046875 0.023071289 0.020996094 -0.07861328 -0.008056641 0.01953125 -0.005554199 0.041503906 0.027832031 0.01361084 0.03466797 -0.18261719 0.12011719 0.07421875 -0.041015625 -0.0099487305 0.04296875 -0.007293701 0.123046875 0.057617188 -0.053466797 -0.032226562 -0.009094238 -0.04663086 0.043945312 -0.05078125 0.068847656 0.0029907227 -0.004180908 -0.044189453 0.07373047 -0.012756348 0.06738281 0.006286621 0.07519531 -0.037841797 0.0048828125 0.044677734 -0.06738281 0.00970459 0.0047302246 0.020507812 0.07128906 0.17089844 0.17382812 0.055664062 0.091308594 -0.037353516 0.049804688 -0.03930664 0.044189453 0.0625 0.048583984 -0.053222656 0.048828125 -0.13085938 -0.028930664 -0.036132812 -0.060791016 -0.057373047 0.123046875 -0.08251953 -0.0119018555 0.125 0.0013580322 0.063964844 -0.10644531 -0.14355469 -0.042236328 0.024047852 -0.16894531 -0.08886719 -0.080566406 0.064941406 0.061279297 -0.04736328 -0.05883789 -0.047607422 0.014465332 -0.0625\n",
            "Vector Length: 300\n",
            "\n",
            "Line 2: for -0.011779785 -0.04736328 0.044677734 0.06347656 -0.018188477 -0.063964844 -0.0013122559 -0.072265625 0.064453125 0.08642578 -0.16992188 -0.039794922 0.07128906 -0.025878906 0.018188477 0.13671875 0.14453125 -0.033691406 -0.09765625 -0.12011719 -0.079589844 0.0625 -0.06689453 0.07421875 0.022705078 0.033447266 -0.18066406 0.052001953 0.0138549805 0.09277344 0.0035095215 -0.009094238 -0.09716797 0.067871094 -0.0087890625 0.044189453 -0.13378906 -0.099609375 0.033203125 0.027954102 0.15527344 -0.017700195 0.014282227 -0.10986328 -0.08544922 -0.07324219 -0.024658203 0.17285156 0.061767578 0.08935547 -0.024291992 0.14160156 -0.032958984 0.02746582 -0.15527344 0.007873535 -0.07080078 0.043701172 0.006011963 -0.055908203 -0.14746094 0.028442383 -0.1328125 -0.17675781 -0.091308594 -0.05078125 -0.026000977 -0.1484375 -0.080566406 0.15039062 -0.04345703 0.07910156 0.033203125 0.09033203 0.022705078 -0.0625 0.1640625 0.0859375 -0.012390137 0.19628906 -0.06225586 0.022460938 -0.030151367 0.021240234 0.003326416 -0.055419922 -0.07324219 0.029785156 0.049804688 0.017456055 0.10449219 0.03881836 0.08496094 -0.24804688 0.06933594 -0.14941406 0.05834961 0.095703125 -0.033447266 0.06298828 0.021362305 -0.14550781 0.053710938 -0.09082031 -0.025390625 0.045410156 0.0053100586 -0.115722656 -0.01953125 0.12109375 0.032226562 0.09472656 -0.064453125 0.022705078 0.12060547 0.060302734 0.12060547 0.048828125 0.09326172 0.06689453 0.029296875 -0.034179688 -0.111328125 0.053466797 -0.025634766 0.017822266 0.06225586 -0.025878906 0.14550781 0.0625 0.107910156 -0.16308594 -0.09765625 -0.10595703 -0.08544922 -0.08886719 0.10107422 -0.079589844 0.008422852 0.024047852 0.13085938 0.05126953 0.08154297 0.09375 -0.05859375 -0.09667969 -0.028320312 -0.14550781 -0.14746094 0.14550781 -0.017578125 0.032958984 -0.08544922 -0.010986328 -0.037109375 -0.013671875 0.035888672 -0.008239746 0.05029297 -0.09472656 0.047851562 0.020751953 0.030639648 0.12988281 0.052734375 0.018798828 -0.017578125 0.03491211 0.018310547 -0.009887695 -0.18457031 -0.08984375 -0.029052734 -0.060791016 -0.05126953 -0.0023651123 0.06640625 -0.08251953 -0.040039062 0.096191406 -0.15429688 -0.15332031 0.028320312 0.013122559 0.029907227 -0.012145996 -0.09667969 0.024780273 0.19335938 0.013000488 0.024169922 -0.035888672 0.09863281 -0.09667969 -0.20019531 -0.013793945 0.0859375 -0.080078125 -0.17675781 -0.17480469 0.005126953 -0.03491211 -0.0546875 0.09375 -0.09326172 -0.011962891 -0.0005645752 0.09765625 0.024780273 -0.039794922 0.009765625 0.11816406 0.025756836 0.123046875 0.064453125 0.07080078 0.029296875 -0.049560547 -0.078125 0.028930664 0.045654297 -0.04296875 0.025878906 -0.051757812 0.140625 0.004272461 -0.037841797 0.02746582 0.060058594 0.028320312 0.028076172 -0.036621094 0.13085938 -9.679794e-05 -0.06933594 -0.022094727 0.067871094 -0.02331543 -0.015319824 -0.05834961 0.061035156 0.00064468384 0.0039978027 -0.07128906 0.091796875 0.026245117 0.020019531 0.03540039 -0.057861328 -0.029663086 0.02734375 0.025146484 0.060302734 0.13183594 -0.0043640137 0.0027313232 0.059814453 0.09863281 -0.091796875 -0.045898438 -0.017456055 0.038330078 -0.019165039 0.04638672 0.047851562 0.09814453 -0.040283203 0.09423828 -0.03466797 -0.042236328 0.0703125 -0.013671875 0.10644531 0.016479492 0.13183594 -0.0016937256 -0.008483887 -0.14257812 -0.04663086 -0.10986328 0.08203125 -0.041015625 -0.018920898 0.087890625 -0.0028076172 0.23828125 -0.04711914 -0.022949219 0.040771484 0.029296875 -0.022583008 0.0037231445 -0.08251953 0.08154297 0.00793457 0.00047683716 0.018432617 0.07128906 -0.03491211 0.024169922\n",
            "Vector Length: 300\n",
            "\n",
            "Line 3: that -0.01574707 -0.028320312 0.083496094 0.05029297 -0.11035156 0.03173828 -0.014221191 -0.08984375 0.11767578 0.11816406 -0.071777344 -0.07714844 -0.068847656 0.07714844 -0.13867188 0.006500244 0.010986328 -0.015136719 -0.0009613037 -0.030273438 -0.00015830994 0.038330078 -0.024169922 -0.045898438 0.09472656 -0.05517578 -0.064941406 0.0061035156 0.0008544922 0.06201172 -0.05444336 0.014099121 0.022216797 -0.044921875 0.111328125 -0.03857422 0.05126953 0.025146484 0.016967773 0.06298828 0.13769531 0.13574219 0.06542969 -0.064453125 -0.024047852 -0.013366699 -0.037109375 0.0043029785 -0.01574707 0.019042969 0.10839844 0.044677734 -0.044921875 -0.095214844 0.08691406 0.08203125 0.0068359375 -0.13183594 0.0027313232 -0.075683594 0.022460938 0.171875 -0.048583984 0.038330078 -0.088378906 -0.017211914 0.021850586 0.13378906 0.010681152 0.049804688 0.038085938 -0.0052490234 0.061279297 -0.050048828 -0.10595703 0.055664062 0.15429688 0.17089844 0.048095703 0.12695312 0.08154297 -0.1015625 0.043945312 -0.013549805 -0.13378906 -0.15722656 0.024291992 0.20898438 -0.10205078 -0.09375 0.048583984 0.07519531 0.0035247803 -0.123535156 -0.024169922 -0.0013198853 0.038330078 0.007598877 0.017700195 0.043701172 -0.09814453 -0.05810547 0.014099121 0.041015625 -0.03540039 -0.022949219 -0.13085938 -0.14453125 0.028930664 -0.122558594 -0.07128906 -0.0071411133 -0.09667969 0.05859375 0.104003906 0.026367188 0.0075683594 0.036132812 0.040283203 0.052734375 -0.20410156 -0.033447266 -0.029052734 0.03173828 -0.123535156 -0.06738281 0.01586914 -0.08642578 0.012512207 -0.06298828 -0.12060547 0.0234375 -0.13183594 -0.11816406 -0.012145996 -0.10986328 0.095214844 -0.041992188 0.018310547 0.051757812 0.014953613 -0.15917969 -0.021728516 -0.005126953 0.035888672 -0.003036499 -0.078125 -0.05126953 -0.0074157715 0.087402344 -0.023925781 0.044189453 -0.11376953 0.021484375 -0.003829956 -0.04345703 -0.104003906 -0.18554688 -0.044921875 0.044921875 -0.044433594 0.019042969 -0.020996094 0.0134887695 0.015258789 -0.030395508 0.04663086 0.10253906 -0.0138549805 0.008239746 -0.052734375 0.01965332 -0.076171875 -0.08984375 0.043945312 0.00793457 0.012634277 -0.10839844 -0.018554688 -0.10449219 -0.15234375 -0.07910156 -0.028808594 -0.018676758 0.09716797 -0.008361816 -0.03540039 0.007537842 0.05810547 0.08203125 0.028808594 0.05102539 0.0031738281 -0.018554688 0.014953613 -0.072753906 -0.034179688 0.043945312 -0.048828125 -0.16113281 0.025634766 0.036621094 -0.00015354156 -0.0012588501 -0.039794922 -0.032226562 -0.01574707 0.060791016 0.09716797 -0.015014648 -0.033691406 -0.015319824 0.0046691895 0.032958984 -0.08203125 0.10644531 0.15429688 0.0087890625 -0.12011719 0.061279297 0.05859375 0.03881836 -0.015991211 -0.043945312 0.12011719 -0.07519531 0.125 0.03515625 0.072753906 0.087402344 0.021118164 -0.07373047 0.034423828 -0.09423828 0.01159668 -0.057128906 -0.07861328 0.015991211 0.075683594 0.030517578 0.0046081543 -0.14550781 -0.024780273 -0.107910156 0.068847656 0.05810547 0.125 -0.06542969 0.005279541 0.01184082 0.06982422 0.12695312 0.06542969 -0.017333984 0.119140625 -0.032470703 0.14453125 0.076660156 -0.032226562 -0.06591797 0.06298828 -0.0625 -0.096191406 0.10644531 -0.039794922 0.11621094 -0.00970459 -0.03540039 -0.06542969 0.05883789 0.16210938 0.05126953 0.15917969 0.095214844 0.076171875 -0.091796875 0.025146484 -0.07861328 0.08935547 -0.05859375 -0.040039062 0.045898438 0.03100586 0.0390625 0.03564453 -0.10595703 -0.037109375 -0.16113281 0.021362305 0.0012207031 -0.011291504 -0.015625 -0.033447266 -0.020629883 -0.01940918 0.063964844 0.020141602 0.006866455 0.061035156 -0.1484375\n",
            "Vector Length: 300\n",
            "\n",
            "Line 4: is 0.0070495605 -0.07324219 0.171875 0.022583008 -0.1328125 0.19824219 0.11279297 -0.107910156 0.071777344 0.020874023 -0.123046875 -0.05908203 0.10107422 0.0107421875 0.14355469 0.25976562 -0.036376953 0.18554688 -0.07861328 -0.022705078 -0.12060547 0.17773438 0.049560547 0.017211914 0.079589844 -0.045654297 -0.18847656 0.18945312 -0.02319336 0.06298828 0.09765625 -0.019042969 -0.07910156 0.15234375 0.17382812 0.1015625 -0.16308594 0.114746094 0.10058594 -0.09277344 0.109375 0.05883789 -0.021606445 0.06347656 0.041992188 -0.008850098 0.032226562 0.10644531 0.064453125 -0.118652344 0.030517578 0.06689453 0.12207031 -0.08300781 0.171875 0.07861328 0.095214844 -0.0077819824 0.02319336 0.0234375 -0.016845703 0.15527344 -0.10986328 -0.17675781 -0.11621094 0.0234375 -0.010620117 0.052734375 -0.13378906 0.079589844 0.07373047 0.043945312 0.115234375 -0.020629883 0.07470703 -0.0115356445 0.080566406 0.041748047 0.080078125 0.3515625 0.09667969 -0.21289062 0.16503906 -0.078125 0.06982422 -0.0013961792 -0.091308594 0.12988281 0.25195312 -0.016113281 0.09326172 -0.14648438 -0.0015106201 -0.15136719 -0.026855469 -0.15722656 0.026367188 0.0859375 0.071777344 0.07714844 -0.0390625 0.05444336 -0.12792969 0.091308594 -0.18457031 -0.037597656 -0.027954102 -0.08984375 -0.11669922 -0.09863281 0.048095703 -0.16210938 -0.10888672 0.08496094 -0.045654297 0.15820312 -0.038085938 -0.08203125 0.203125 0.08642578 0.06933594 0.032226562 -0.16015625 0.09472656 -0.024658203 0.05419922 0.027954102 0.044921875 0.16992188 0.072753906 -0.036376953 -0.010253906 -0.017089844 -0.107421875 -0.0007019043 -0.07373047 0.25390625 0.056640625 0.03515625 -0.008605957 0.18554688 0.021484375 0.26367188 -0.023803711 -0.099121094 -0.041259766 -0.06933594 -0.11376953 0.050048828 -0.05883789 0.046142578 0.087402344 0.10546875 0.10644531 0.027954102 0.09472656 0.11621094 -0.17285156 -0.03491211 -0.20800781 0.059570312 0.104003906 -0.0017929077 0.05859375 -0.029785156 -0.037597656 0.048583984 -0.063964844 0.079589844 0.06933594 -0.10498047 -0.14453125 0.04345703 -0.068847656 -0.03564453 -0.01171875 0.013671875 -0.06591797 0.119140625 0.03125 -0.04638672 -0.0019683838 0.0073547363 -0.056640625 0.027832031 0.08251953 -0.0134887695 0.071777344 0.14453125 0.12792969 0.042236328 0.14160156 -0.018066406 0.021606445 -0.091796875 0.13378906 -0.1953125 -0.05029297 -0.037841797 -0.096191406 0.103027344 -0.106933594 -0.14746094 0.099609375 -0.23046875 0.22753906 -0.07519531 0.064941406 0.091796875 0.046875 0.06298828 0.06982422 0.046142578 0.09716797 -0.20214844 0.19921875 0.18652344 -0.119628906 -0.14257812 0.15039062 -0.033691406 -0.14550781 -0.0006904602 -0.07324219 0.13378906 0.03564453 -0.022949219 0.027709961 -0.07910156 0.20703125 -0.083496094 -0.049560547 0.03149414 0.1484375 0.055664062 -0.044921875 -0.079589844 0.004760742 -0.020751953 0.060058594 0.004760742 0.011169434 0.17285156 -0.13476562 0.030761719 -0.079589844 0.09033203 0.061035156 0.07714844 -0.05029297 -0.092285156 -0.26757812 0.107910156 0.0859375 0.06298828 0.107910156 -0.026733398 0.10205078 -0.12060547 0.052978516 0.09472656 -0.16503906 0.044189453 0.072265625 0.041259766 0.42578125 -0.103027344 -0.16015625 -0.09033203 -0.063964844 -0.048095703 0.14453125 0.06542969 0.049316406 0.05419922 0.13574219 -0.01928711 -0.21582031 -0.07421875 -0.14648438 0.011474609 -0.16503906 -0.10498047 0.0032043457 0.13476562 -0.003967285 -0.103515625 -0.13964844 0.10449219 -0.012573242 -0.23339844 -0.036376953 -0.09375 0.18261719 0.02709961 0.12792969 -0.024780273 0.011230469 0.1640625 0.106933594\n",
            "Vector Length: 300\n",
            "\n",
            "\n",
            "Successfully loaded 18920 vectors into the dictionary.\n"
          ]
        }
      ],
      "source": [
        "# --- 2. Read and Process the Word Vectors (Replaces file access from Google Drive) ---\n",
        "# On your local machine, the file is in the current working directory,\n",
        "# so you can open it directly by its name.\n",
        "\n",
        "word2vec_data = {}\n",
        "lines_to_print = 4\n",
        "\n",
        "print(f\"\\nProcessing file and printing first {lines_to_print} lines:\")\n",
        "try:\n",
        "    with open(filename, encoding=\"utf-8\") as input_file:\n",
        "      i = 0\n",
        "      for line in input_file:\n",
        "        # Printing the first 4 lines as requested\n",
        "        if i < lines_to_print:\n",
        "          print(f\"Line {i+1}: {line.strip()}\")\n",
        "          # To check the vector length (skip first word/token)\n",
        "          print(f\"Vector Length: {len(line.split()[1:])}\\n\")\n",
        "        \n",
        "        # Converting to a dictionary format\n",
        "        line_parts = line.split()\n",
        "        if line_parts: # Ensure the line is not empty\n",
        "          word = line_parts[0]\n",
        "          # Convert the rest of the parts (the vector) to numpy float32\n",
        "          vector = [np.float32(x) for x in line_parts[1:]]\n",
        "          word2vec_data[word] = vector\n",
        "        \n",
        "        i += 1\n",
        "        \n",
        "        # Optional: Stop after a few lines for testing to save memory/time\n",
        "        # if i > 1000: \n",
        "        #    break \n",
        "        \n",
        "    print(f\"\\nSuccessfully loaded {len(word2vec_data)} vectors into the dictionary.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nERROR: The file '{filename}' was not found. Please check if the download was successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred while reading the file: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "our embed dim is 300\n",
            "Vocabulary size: 18922\n",
            "300\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "word2vec_vocab = Vocabulary()\n",
        "vectors = []\n",
        "\n",
        "# taking the embedding dimension for the word2vec words\n",
        "embedding_dim = 300 #len(word2vec_data.values()[0]) # recheck\n",
        "print(f'our embed dim is {embedding_dim}')\n",
        "# creating a tensor with values distributed based on the normal distribution with mean 0 and variance 1\n",
        "# this is because for the unkown case we should be giving the embedding a\n",
        "# somewhat realistic distribution to an actual word\n",
        "# unk_vector = torch.randn(embedding_dim)\n",
        "unk_vector = np.random.randn(embedding_dim)\n",
        "vectors.append(unk_vector)\n",
        "word2vec_vocab.add_token('<unk>')\n",
        "# for padding, however, we do not want the embedding values to distort the\n",
        "# meaning of our sentence. Hence, it is better to apply zero-padding.\n",
        "pad_vector = np.zeros(embedding_dim)\n",
        "vectors.append(pad_vector)\n",
        "\n",
        "word2vec_vocab.add_token('<pad>')\n",
        "\n",
        "for token, embedding in word2vec_data.items():\n",
        "  #print(token)\n",
        "  #print(embedding)\n",
        "  word2vec_vocab.add_token(token)\n",
        "  vectors.append(np.array(embedding))\n",
        "  #break\n",
        "\n",
        "#word2vec_vocab.build()\n",
        "print(\"Vocabulary size:\", len(word2vec_vocab.w2i))\n",
        "print(embedding_dim)\n",
        "vectors = np.stack(vectors, axis=0) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC-7mRyYNG9b"
      },
      "source": [
        "#### Exercise: words not in our pre-trained set\n",
        "\n",
        "How many words in the training, dev, and test set are also in your vector set?\n",
        "How many words are not there?\n",
        "\n",
        "Store the words that are not in the word vector set in the set below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9f4b45BXKFC"
      },
      "source": [
        "#### Exercise: Finish the LSTM cell below.\n",
        "You will need to implement the LSTM formulas:\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "        i = \\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\n",
        "        f = \\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\n",
        "        g = \\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\\n",
        "        o = \\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\n",
        "        c' = f * c + i * g \\\\\n",
        "        h' = o \\tanh(c') \\\\\n",
        "\\end{array}\n",
        " $$\n",
        "\n",
        "where $\\sigma$ is the sigmoid function.\n",
        "\n",
        "*Note that the LSTM formulas can differ slightly between different papers. We use the PyTorch LSTM formulation here.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJ9m5kLMd7-v"
      },
      "outputs": [],
      "source": [
        "class MyLSTMCell(nn.Module):\n",
        "  \"\"\"Our own LSTM cell\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, bias=True):\n",
        "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
        "    super(MyLSTMCell, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bias = bias\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    self.W_ii = torch.nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "    self.W_if = torch.nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "    self.W_ig = torch.nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "    self.W_io = torch.nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "    self.W_hi = torch.nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.W_hf = torch.nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.W_hg = torch.nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.W_ho = torch.nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.b_ii = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
        "    self.b_if = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
        "    self.b_ig = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
        "    self.b_io = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
        "    self.b_hi = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
        "    self.b_hf = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
        "    self.b_hg = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
        "    self.b_ho = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
        "    # end of my code\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    \"\"\"This is PyTorch's default initialization method\"\"\"\n",
        "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "    for weight in self.parameters():\n",
        "      weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "  def forward(self, input_, hx, mask=None):\n",
        "    \"\"\"\n",
        "    input is (batch, input_size)\n",
        "    hx is ((batch, hidden_size), (batch, hidden_size))\n",
        "    \"\"\"\n",
        "    prev_h, prev_c = hx\n",
        "\n",
        "    # project input and prev state\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    i = torch.sigmoid(torch.matmul(input_, self.W_ii) + self.b_ii + torch.matmul(prev_h, self.W_hi) + self.b_hi)\n",
        "\n",
        "    #raise NotImplementedError(\"Implement this\")\n",
        "\n",
        "    # main LSTM computation\n",
        "\n",
        "    # i = ...\n",
        "    f = torch.sigmoid(torch.matmul(input_, self.W_if) + self.b_if + torch.matmul(prev_h, self.W_hf) + self.b_hf)\n",
        "    g = torch.tanh(torch.matmul(input_, self.W_ig) + self.b_ig + torch.matmul(prev_h, self.W_hg) + self.b_hg)\n",
        "    o = torch.sigmoid(torch.matmul(input_, self.W_io) + self.b_io + torch.matmul(prev_h, self.W_ho) + self.b_ho)\n",
        "    c = f * prev_c + i * g\n",
        "    h = o * torch.tanh(c)\n",
        "\n",
        "    return h, c\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"{}({:d}, {:d})\".format(\n",
        "        self.__class__.__name__, self.input_size, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iuYZm5poEn5"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "  \"\"\"Encodes sentence with an LSTM and projects final hidden state\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
        "    super(LSTMClassifier, self).__init__()\n",
        "    self.vocab = vocab\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
        "    self.rnn = MyLSTMCell(embedding_dim, hidden_dim)\n",
        "\n",
        "    self.output_layer = nn.Sequential(\n",
        "        nn.Dropout(p=0.5),  # explained later\n",
        "        nn.Linear(hidden_dim, output_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    B = x.size(0)  # batch size (this is 1 for now, i.e. 1 single example)\n",
        "    T = x.size(1)  # timesteps (the number of words in the sentence)\n",
        "\n",
        "    input_ = self.embed(x)\n",
        "\n",
        "    # here we create initial hidden states containing zeros\n",
        "    # we use a trick here so that, if input is on the GPU, then so are hx and cx\n",
        "    hx = input_.new_zeros(B, self.rnn.hidden_size)\n",
        "    cx = input_.new_zeros(B, self.rnn.hidden_size)\n",
        "\n",
        "    # process input sentences one word/timestep at a time\n",
        "    # input is batch-major (i.e., batch size is the first dimension)\n",
        "    # so the first word(s) is (are) input_[:, 0]\n",
        "    outputs = []\n",
        "    for i in range(T):\n",
        "      hx, cx = self.rnn(input_[:, i], (hx, cx))\n",
        "      outputs.append(hx)\n",
        "\n",
        "    # if we have a single example, our final LSTM state is the last hx\n",
        "    if B == 1:\n",
        "      final = hx\n",
        "    else:\n",
        "      #\n",
        "      # This part is explained in next section, ignore this else-block for now.\n",
        "      #\n",
        "      # We processed sentences with different lengths, so some of the sentences\n",
        "      # had already finished and we have been adding padding inputs to hx.\n",
        "      # We select the final state based on the length of each sentence.\n",
        "\n",
        "      # two lines below not needed if using LSTM from pytorch\n",
        "      outputs = torch.stack(outputs, dim=0)           # [T, B, D]\n",
        "      outputs = outputs.transpose(0, 1).contiguous()  # [B, T, D]\n",
        "\n",
        "      # to be super-sure we're not accidentally indexing the wrong state\n",
        "      # we zero out positions that are invalid\n",
        "      pad_positions = (x == 1).unsqueeze(-1)\n",
        "\n",
        "      outputs = outputs.contiguous()\n",
        "      outputs = outputs.masked_fill_(pad_positions, 0.)\n",
        "\n",
        "      mask = (x != 1)  # true for valid positions [B, T]\n",
        "      lengths = mask.sum(dim=1)                 # [B, 1]\n",
        "\n",
        "      indexes = (lengths - 1) + torch.arange(B, device=x.device, dtype=x.dtype) * T\n",
        "      final = outputs.view(-1, self.hidden_dim)[indexes]  # [B, D]\n",
        "\n",
        "    # we use the last hidden state to classify the sentence\n",
        "    logits = self.output_layer(final)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgZoSPD4fsf_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# lstm_model = LSTMClassifier(len(word2vec_vocab.w2i), 300, 168, len(t2i), word2vec_vocab)\n",
        "\n",
        "# # copy pre-trained word vectors into embeddings table\n",
        "# with torch.no_grad():\n",
        "#   lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "#   lstm_model.embed.weight.requires_grad = False\n",
        "\n",
        "# print(lstm_model)\n",
        "# print_parameters(lstm_model)\n",
        "\n",
        "# lstm_model = lstm_model.to(device)\n",
        "# optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
        "\n",
        "# lstm_losses, lstm_accuracies = train_model(\n",
        "#     lstm_model, optimizer, num_iterations=25000,\n",
        "#     print_every=250, eval_every=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "IoAE2JBiXJ3P"
      },
      "outputs": [],
      "source": [
        "def get_minibatch(data, batch_size=25, shuffle=True):\n",
        "  \"\"\"Return minibatches, optional shuffling\"\"\"\n",
        "\n",
        "  if shuffle:\n",
        "    print(\"Shuffling training data\")\n",
        "    random.shuffle(data)  # shuffle training data each epoch\n",
        "\n",
        "  batch = []\n",
        "\n",
        "  # yield minibatches\n",
        "  for example in data:\n",
        "    batch.append(example)\n",
        "\n",
        "    if len(batch) == batch_size:\n",
        "      yield batch\n",
        "      batch = []\n",
        "\n",
        "  # in case there is something left\n",
        "  if len(batch) > 0:\n",
        "    yield batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwZM-XYkT8Zx"
      },
      "source": [
        "#### Padding function\n",
        "We will need a function that adds padding 1s to a sequence of IDs so that\n",
        "it becomes as long as the longest sequence in the minibatch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp0sK1ghw4Ft",
        "outputId": "b6045f4f-3159-4673-ffae-9fcde29806de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 3, 4, 1, 1]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def pad(tokens, length, pad_value=1):\n",
        "  \"\"\"add padding 1s to a sequence to that it has the desired length\"\"\"\n",
        "  return tokens + [pad_value] * (length - len(tokens))\n",
        "\n",
        "# example\n",
        "tokens = [2, 3, 4]\n",
        "pad(tokens, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL2iixMYUgfh"
      },
      "source": [
        "#### New `prepare` function\n",
        "\n",
        "We will also need a new function that turns a mini-batch into PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ZID0cqozWks8"
      },
      "outputs": [],
      "source": [
        "def prepare_minibatch(mb, vocab):\n",
        "  \"\"\"\n",
        "  Minibatch is a list of examples.\n",
        "  This function converts words to IDs and returns\n",
        "  torch tensors to be used as input/targets.\n",
        "  \"\"\"\n",
        "  batch_size = len(mb)\n",
        "  maxlen = max([len(ex.tokens) for ex in mb])\n",
        "\n",
        "  # vocab returns 0 if the word is not there\n",
        "  x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen) for ex in mb]\n",
        "\n",
        "  x = torch.LongTensor(x)\n",
        "  x = x.to(device)\n",
        "\n",
        "  y = [ex.label for ex in mb]\n",
        "  y = torch.LongTensor(y)\n",
        "  y = y.to(device)\n",
        "\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiM8l1vPLM1q",
        "outputId": "1b135825-b9c3-4ef5-8ea3-4f2c1301ae0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example(tokens=['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.'], tree=Tree('3', [Tree('2', [Tree('2', ['The']), Tree('2', ['Rock'])]), Tree('4', [Tree('3', [Tree('2', ['is']), Tree('4', [Tree('2', ['destined']), Tree('2', [Tree('2', [Tree('2', [Tree('2', [Tree('2', ['to']), Tree('2', [Tree('2', ['be']), Tree('2', [Tree('2', ['the']), Tree('2', [Tree('2', ['21st']), Tree('2', [Tree('2', [Tree('2', ['Century']), Tree('2', [\"'s\"])]), Tree('2', [Tree('3', ['new']), Tree('2', [Tree('2', ['``']), Tree('2', ['Conan'])])])])])])])]), Tree('2', [\"''\"])]), Tree('2', ['and'])]), Tree('3', [Tree('2', ['that']), Tree('3', [Tree('2', ['he']), Tree('3', [Tree('2', [\"'s\"]), Tree('3', [Tree('2', ['going']), Tree('3', [Tree('2', ['to']), Tree('4', [Tree('3', [Tree('2', ['make']), Tree('3', [Tree('3', [Tree('2', ['a']), Tree('3', ['splash'])]), Tree('2', [Tree('2', ['even']), Tree('3', ['greater'])])])]), Tree('2', [Tree('2', ['than']), Tree('2', [Tree('2', [Tree('2', [Tree('2', [Tree('1', [Tree('2', ['Arnold']), Tree('2', ['Schwarzenegger'])]), Tree('2', [','])]), Tree('2', [Tree('2', ['Jean-Claud']), Tree('2', [Tree('2', ['Van']), Tree('2', ['Damme'])])])]), Tree('2', ['or'])]), Tree('2', [Tree('2', ['Steven']), Tree('2', ['Segal'])])])])])])])])])])])])]), Tree('2', ['.'])])]), label=3, transitions=[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1])\n",
            "Example(tokens=['The', 'gorgeously', 'elaborate', 'continuation', 'of', '``', 'The', 'Lord', 'of', 'the', 'Rings', \"''\", 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'can', 'not', 'adequately', 'describe', 'co-writer/director', 'Peter', 'Jackson', \"'s\", 'expanded', 'vision', 'of', 'J.R.R.', 'Tolkien', \"'s\", 'Middle-earth', '.'], tree=Tree('4', [Tree('4', [Tree('4', [Tree('2', ['The']), Tree('4', [Tree('3', ['gorgeously']), Tree('3', [Tree('2', ['elaborate']), Tree('2', ['continuation'])])])]), Tree('2', [Tree('2', [Tree('2', ['of']), Tree('2', ['``'])]), Tree('2', [Tree('2', ['The']), Tree('2', [Tree('2', [Tree('2', ['Lord']), Tree('2', [Tree('2', ['of']), Tree('2', [Tree('2', ['the']), Tree('2', ['Rings'])])])]), Tree('2', [Tree('2', [\"''\"]), Tree('2', ['trilogy'])])])])])]), Tree('2', [Tree('3', [Tree('2', [Tree('2', ['is']), Tree('2', [Tree('2', ['so']), Tree('2', ['huge'])])]), Tree('2', [Tree('2', ['that']), Tree('3', [Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', ['column'])]), Tree('2', [Tree('2', ['of']), Tree('2', ['words'])])]), Tree('2', [Tree('2', [Tree('2', [Tree('2', ['can']), Tree('1', ['not'])]), Tree('3', ['adequately'])]), Tree('2', [Tree('2', ['describe']), Tree('2', [Tree('3', [Tree('2', [Tree('2', ['co-writer/director']), Tree('2', [Tree('2', ['Peter']), Tree('3', [Tree('2', ['Jackson']), Tree('2', [\"'s\"])])])]), Tree('3', [Tree('2', ['expanded']), Tree('2', ['vision'])])]), Tree('2', [Tree('2', ['of']), Tree('2', [Tree('2', [Tree('2', ['J.R.R.']), Tree('2', [Tree('2', ['Tolkien']), Tree('2', [\"'s\"])])]), Tree('2', ['Middle-earth'])])])])])])])])]), Tree('2', ['.'])])]), label=4, transitions=[0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1])\n",
            "Example(tokens=['Singer/composer', 'Bryan', 'Adams', 'contributes', 'a', 'slew', 'of', 'songs', '--', 'a', 'few', 'potential', 'hits', ',', 'a', 'few', 'more', 'simply', 'intrusive', 'to', 'the', 'story', '--', 'but', 'the', 'whole', 'package', 'certainly', 'captures', 'the', 'intended', ',', 'er', ',', 'spirit', 'of', 'the', 'piece', '.'], tree=Tree('3', [Tree('3', [Tree('2', [Tree('2', [Tree('2', [Tree('2', [Tree('2', ['Singer/composer']), Tree('2', [Tree('2', ['Bryan']), Tree('2', ['Adams'])])]), Tree('2', [Tree('2', ['contributes']), Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', ['slew'])]), Tree('2', [Tree('2', ['of']), Tree('2', ['songs'])])])])]), Tree('2', [Tree('2', ['--']), Tree('2', [Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', [Tree('2', ['few']), Tree('3', ['potential'])])]), Tree('2', [Tree('2', [Tree('2', ['hits']), Tree('2', [','])]), Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', ['few'])]), Tree('1', [Tree('1', [Tree('2', ['more']), Tree('1', [Tree('2', ['simply']), Tree('2', ['intrusive'])])]), Tree('2', [Tree('2', ['to']), Tree('2', [Tree('2', ['the']), Tree('2', ['story'])])])])])])]), Tree('2', ['--'])])])]), Tree('2', ['but'])]), Tree('3', [Tree('4', [Tree('2', ['the']), Tree('3', [Tree('2', ['whole']), Tree('2', ['package'])])]), Tree('2', [Tree('3', ['certainly']), Tree('3', [Tree('2', ['captures']), Tree('2', [Tree('1', [Tree('2', ['the']), Tree('2', [Tree('2', [Tree('2', ['intended']), Tree('2', [Tree('2', [',']), Tree('2', [Tree('2', ['er']), Tree('2', [','])])])]), Tree('3', ['spirit'])])]), Tree('2', [Tree('2', ['of']), Tree('2', [Tree('2', ['the']), Tree('2', ['piece'])])])])])])])]), Tree('2', ['.'])]), label=3, transitions=[0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1])\n"
          ]
        }
      ],
      "source": [
        "mb = next(get_minibatch(train_data, batch_size=3, shuffle=False))\n",
        "for ex in mb:\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg8zEK8zyUCH",
        "outputId": "0ff71390-10fa-4b29-9f4d-a1ced7566ce9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x tensor([[  14, 1098,   10, 3554,    8,   27,    4, 2912, 3555,    9,  151,   60,\n",
            "         5869,   61,    5,   11,   82,    9,  290,    8,   84,    6, 5870,   75,\n",
            "         3556,   38, 1887, 2913,    3, 8737, 2139, 5871,   48,  828, 8738,    2,\n",
            "            1,    1,    1],\n",
            "        [  14, 2914, 2140, 5872,    7,   60,   14, 4473,    7,    4, 5873,   61,\n",
            "         5874,   10,   49,  878,   11,    6, 5875,    7,  679,   65,   31, 3557,\n",
            "         2141, 8739, 1099,  829,    9, 8740,  629,    7, 8741, 5876,    9, 8742,\n",
            "            2,    1,    1],\n",
            "        [8743, 5877, 2142, 8744,    6, 8745,    7, 1684,   33,    6,  171,  752,\n",
            "         1269,    3,    6,  171,   37,  316, 8746,    8,    4,   52,   33,   17,\n",
            "            4,  306, 3558,  377,  680,    4, 1021,    3, 4474,    3,  512,    7,\n",
            "            4,  277,    2]])\n",
            "y tensor([3, 4, 3])\n"
          ]
        }
      ],
      "source": [
        "# We should find padding 1s at the end\n",
        "x, y = prepare_minibatch(mb, v)\n",
        "print(\"x\", x)\n",
        "print(\"y\", y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYBJEoSNUwI0"
      },
      "source": [
        "#### Evaluate (mini-batch version)\n",
        "\n",
        "We can now update our evaluation function to use mini-batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "eiZZpEghzqou"
      },
      "outputs": [],
      "source": [
        "def evaluate_root(model, data, batch_fn=get_minibatch, prep_fn=prepare_minibatch, batch_size=16):\n",
        "    \"\"\"Compute accuracy using only root node predictions.\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()  # disable dropout\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for mb in batch_fn(data, batch_size=batch_size, shuffle=False):\n",
        "            (x, transitions), targets, _ = prep_fn(mb, model.vocab) # get embeddings and labels\n",
        "            root_logits, _ = model((x, transitions))        # forward pass\n",
        "\n",
        "            # Use only root hidden states for prediction\n",
        "            logits = model.output_layer(root_logits)  # [B, num_classes]\n",
        "            predictions = logits.argmax(dim=-1)       # [B]\n",
        "            targets = targets.view(-1)               # [B]\n",
        "\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    accuracy = correct / float(total)\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23wAZomozh_2"
      },
      "source": [
        "# LSTM (Mini-batched)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-gkPU7jzBe2"
      },
      "source": [
        "With this, let's run the LSTM again but now using mini-batches!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pg0Xumc3ZUS",
        "outputId": "9424de97-f1f5-4a04-bd20-09d318706a6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              3                                                                     \n",
            "  ____________|____________________                                                  \n",
            " |                                 4                                                \n",
            " |        _________________________|______________________________________________   \n",
            " |       4                                                                        | \n",
            " |    ___|______________                                                          |  \n",
            " |   |                  4                                                         | \n",
            " |   |         _________|__________                                               |  \n",
            " |   |        |                    3                                              | \n",
            " |   |        |               _____|______________________                        |  \n",
            " |   |        |              |                            4                       | \n",
            " |   |        |              |            ________________|_______                |  \n",
            " |   |        |              |           |                        2               | \n",
            " |   |        |              |           |                 _______|___            |  \n",
            " |   |        3              |           |                |           2           | \n",
            " |   |    ____|_____         |           |                |        ___|_____      |  \n",
            " |   |   |          4        |           3                |       2         |     | \n",
            " |   |   |     _____|___     |      _____|_______         |    ___|___      |     |  \n",
            " 2   2   2    3         2    2     3             2        2   2       2     2     2 \n",
            " |   |   |    |         |    |     |             |        |   |       |     |     |  \n",
            " It  's  a  lovely     film with lovely     performances  by Buy     and Accorsi  . \n",
            "\n",
            "Transitions:\n",
            "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3888340663.py:2: DeprecationWarning: \n",
            "    Class TreePrettyPrinter has been deprecated.  Import\n",
            "    `TreePrettyPrinter` using `from nltk.tree import\n",
            "    TreePrettyPrinter` instead.\n",
            "  print(TreePrettyPrinter(ex.tree))\n"
          ]
        }
      ],
      "source": [
        "ex = next(examplereader(\"trees/dev.txt\"))\n",
        "print(TreePrettyPrinter(ex.tree))\n",
        "print(\"Transitions:\")\n",
        "print(ex.transitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y069gM4_v64",
        "outputId": "efd81144-f401-420c-c9e4-6be60f97e705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S S S S S R R S S S R S S S R S R R R R R R S R R\n",
            "0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1\n"
          ]
        }
      ],
      "source": [
        "# for comparison\n",
        "seq = ex.transitions\n",
        "s = \" \".join([\"S\" if t == 0 else \"R\" for t in seq])\n",
        "print(s)\n",
        "print(\" \".join(map(str, seq)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-qOuKbDAiBn"
      },
      "source": [
        "## Coding the Tree-LSTM\n",
        "\n",
        "The code below contains a Binary Tree-LSTM cell.\n",
        "It is used in the TreeLSTM class below it, which in turn is used in the TreeLSTMClassifier.\n",
        "The job of the TreeLSTM class is to encode a complete sentence and return the root node.\n",
        "The job of the TreeLSTMCell is to return a new state when provided with two children (a reduce action). By repeatedly calling the TreeLSTMCell, the TreeLSTM will encode a sentence. This can be done for multiple sentences at the same time.\n",
        "\n",
        "\n",
        "#### Exercise\n",
        "Check the `forward` function and complete the Tree-LSTM formulas.\n",
        "You can see that we defined a large linear layer for you, that projects the *concatenation* of the left and right child into the input gate, left forget gate, right forget gate, candidate, and output gate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "J9b9mjMlN7Pb"
      },
      "outputs": [],
      "source": [
        "class TreeLSTMCell(nn.Module):\n",
        "  \"\"\"A Binary Tree LSTM cell\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, bias=True):\n",
        "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
        "    super(TreeLSTMCell, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bias = bias\n",
        "\n",
        "    self.reduce_layer = nn.Linear(2 * hidden_size, 5 * hidden_size)\n",
        "    self.dropout_layer = nn.Dropout(p=0.25)\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    \"\"\"This is PyTorch's default initialization method\"\"\"\n",
        "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "    for weight in self.parameters():\n",
        "      weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "  def forward(self, hx_l, hx_r, mask=None):\n",
        "    \"\"\"\n",
        "    hx_l is ((batch, hidden_size), (batch, hidden_size))\n",
        "    hx_r is ((batch, hidden_size), (batch, hidden_size))\n",
        "    \"\"\"\n",
        "    prev_h_l, prev_c_l = hx_l  # left child\n",
        "    prev_h_r, prev_c_r = hx_r  # right child\n",
        "\n",
        "    B = prev_h_l.size(0)\n",
        "\n",
        "    # we concatenate the left and right children\n",
        "    # you can also project from them separately and then sum\n",
        "    children = torch.cat([prev_h_l, prev_h_r], dim=1)\n",
        "\n",
        "    # project the combined children into a 5D tensor for i,fl,fr,g,o\n",
        "    # this is done for speed, and you could also do it separately\n",
        "    proj = self.reduce_layer(children)  # shape: B x 5D\n",
        "\n",
        "    proj = self.dropout_layer(proj)\n",
        "\n",
        "    # each shape: B x D\n",
        "    i, f_l, f_r, g, o = torch.chunk(proj, 5, dim=-1)\n",
        "\n",
        "    # main Tree LSTM computation\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # You only need to complete the commented lines below.\n",
        "    # raise NotImplementedError(\"Implement this.\")\n",
        "\n",
        "    # The shape of each of these is [batch_size, hidden_size]\n",
        "\n",
        "    i = torch.sigmoid(i)\n",
        "    f_l = torch.sigmoid(f_l)\n",
        "    f_r = torch.sigmoid(f_r)\n",
        "    g = torch.tanh(g)\n",
        "\n",
        "    g = self.dropout_layer(g)\n",
        "    o = torch.sigmoid(o)\n",
        "\n",
        "    c = f_l * prev_c_l + f_r * prev_c_r + i * g\n",
        "    h = o * torch.tanh(c)\n",
        "\n",
        "    return h, c\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"{}({:d}, {:d})\".format(\n",
        "        self.__class__.__name__, self.input_size, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5PixvTd4AqsQ"
      },
      "outputs": [],
      "source": [
        "# Helper functions for batching and unbatching states\n",
        "# For speed we want to combine computations by batching, but\n",
        "# for processing logic we want to turn the output into lists again\n",
        "# to easily manipulate.\n",
        "\n",
        "def batch(states):\n",
        "  \"\"\"\n",
        "  Turns a list of states into a single tensor for fast processing.\n",
        "  This function also chunks (splits) each state into a (h, c) pair\"\"\"\n",
        "  return torch.cat(states, 0).chunk(2, 1)\n",
        "\n",
        "def unbatch(state):\n",
        "  \"\"\"\n",
        "  Turns a tensor back into a list of states.\n",
        "  First, (h, c) are merged into a single state.\n",
        "  Then the result is split into a list of sentences.\n",
        "  \"\"\"\n",
        "  return torch.split(torch.cat(state, 1), 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CynltDasaLPt"
      },
      "source": [
        "Take some time to understand the class below, having read the explanation above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "rQOqMXG4gX5G"
      },
      "outputs": [],
      "source": [
        "class TreeLSTM(nn.Module):\n",
        "  \"\"\"Encodes a sentence using a TreeLSTMCell\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, tree_type='N-ary' ,bias=True):\n",
        "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
        "    super(TreeLSTM, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bias = bias\n",
        "    if tree_type == 'N-ary':\n",
        "      self.reduce = TreeLSTMCell(input_size, hidden_size)\n",
        "    elif tree_type == 'child-sum':\n",
        "      print(f\"you have succesfully created chld sum\")\n",
        "      self.reduce = TreeLSTM_child_sum_Cell(input_size, hidden_size)\n",
        "\n",
        "    self.subtrees = []\n",
        "    # project word to initial c\n",
        "    self.proj_x = nn.Linear(input_size, hidden_size)\n",
        "    self.proj_x_gate = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "    self.buffers_dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "  def forward(self, x, transitions):\n",
        "    \"\"\"\n",
        "    WARNING: assuming x is reversed!\n",
        "    :param x: word embeddings [B, T, E]\n",
        "    :param transitions: [2T-1, B]\n",
        "    :return: root states\n",
        "    \"\"\"\n",
        "\n",
        "    B = x.size(0)  # batch size\n",
        "    T = x.size(1)  # time\n",
        "\n",
        "    self.subtrees = [[] for _ in range(B)]\n",
        "\n",
        "    # compute an initial c and h for each word\n",
        "    # Note: this corresponds to input x in the Tai et al. Tree LSTM paper.\n",
        "    # We do not handle input x in the TreeLSTMCell itself.\n",
        "    buffers_c = self.proj_x(x)\n",
        "    buffers_h = buffers_c.tanh()\n",
        "    buffers_h_gate = self.proj_x_gate(x).sigmoid()\n",
        "    buffers_h = buffers_h_gate * buffers_h\n",
        "\n",
        "    # concatenate h and c for each word\n",
        "    buffers = torch.cat([buffers_h, buffers_c], dim=-1)\n",
        "\n",
        "    D = buffers.size(-1) // 2\n",
        "\n",
        "    # we turn buffers into a list of stacks (1 stack for each sentence)\n",
        "    # first we split buffers so that it is a list of sentences (length B)\n",
        "    # then we split each sentence to be a list of word vectors\n",
        "    buffers = buffers.split(1, dim=0)  # Bx[T, 2D]\n",
        "    buffers = [list(b.squeeze(0).split(1, dim=0)) for b in buffers]  # BxTx[2D]\n",
        "\n",
        "    # create B empty stacks\n",
        "    stacks = [[] for _ in buffers]\n",
        "\n",
        "    # t_batch holds 1 transition for each sentence\n",
        "    for t_batch in transitions:\n",
        "\n",
        "      child_l = []  # contains the left child for each sentence with reduce action\n",
        "      child_r = []  # contains the corresponding right child\n",
        "\n",
        "      # iterate over sentences in the batch\n",
        "      # each has a transition t, a buffer and a stack\n",
        "      for transition, buffer, stack in zip(t_batch, buffers, stacks):\n",
        "        if transition == SHIFT:\n",
        "          stack.append(buffer.pop())\n",
        "        elif transition == REDUCE:\n",
        "          assert len(stack) >= 2, \\\n",
        "            \"Stack too small! Should not happen with valid transition sequences\"\n",
        "          child_r.append(stack.pop())  # right child is on top\n",
        "          child_l.append(stack.pop())\n",
        "\n",
        "      # if there are sentences with reduce transition, perform them batched\n",
        "      if child_l:\n",
        "        reduced = iter(unbatch(self.reduce(batch(child_l), batch(child_r))))\n",
        "        #self.subtrees.append(reduced)  # add to list of sub\n",
        "        for idx, (transition, stack) in enumerate(zip(t_batch, stacks)):\n",
        "          if transition == REDUCE:\n",
        "              node = next(reduced)\n",
        "              h, c = node.chunk(2, dim=-1)\n",
        "              self.subtrees[idx].append(h)   # store subtree for sentence idx\n",
        "              stack.append(node)\n",
        "\n",
        "    final = [stack.pop().chunk(2, -1)[0] for stack in stacks]\n",
        "    final = torch.cat(final, dim=0)  # tensor [B, D]\n",
        "\n",
        "    return self.subtrees, final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4EzbVzqaXkw"
      },
      "source": [
        "Just like the LSTM before, we will need an extra class that does the classifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "nLxpYRvtQKge"
      },
      "outputs": [],
      "source": [
        "class TreeLSTMClassifier(nn.Module):\n",
        "  \"\"\"Encodes sentence with a TreeLSTM and projects final hidden state\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab, tree_type='N=ary'):\n",
        "    super(TreeLSTMClassifier, self).__init__()\n",
        "    self.vocab = vocab\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
        "    self.treelstm = TreeLSTM(embedding_dim, hidden_dim, tree_type)\n",
        "    self.output_layer = nn.Sequential(\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(hidden_dim, output_dim, bias=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # x is a pair here of words and transitions; we unpack it here.\n",
        "    # x is batch-major: [B, T], transitions is time major [2T-1, B]\n",
        "    x, transitions = x\n",
        "    emb = self.embed(x)\n",
        "\n",
        "    # we use the root/top state of the Tree LSTM to classify the sentence\n",
        "    subtrees, root_states = self.treelstm(emb, transitions)\n",
        "    # root logits\n",
        "    root_logits = self.output_layer(root_states)  # [B, C]\n",
        "\n",
        "    # subtree logits (variable number per sentence)\n",
        "    subtree_logits = []\n",
        "    for sent_nodes in subtrees:\n",
        "        sent_logits = [ self.output_layer(node) for node in sent_nodes ]\n",
        "        subtree_logits.append(sent_logits)\n",
        "\n",
        "    return root_logits, subtree_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "FE44VwKg1VbA"
      },
      "outputs": [],
      "source": [
        "def evaluate_root(model, data, batch_fn=get_minibatch, prep_fn=prepare_minibatch, batch_size=16):\n",
        "    \"\"\"Compute root-level accuracy.\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()  # disable dropout\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for mb in batch_fn(data, batch_size=batch_size, shuffle=False):\n",
        "            (x, transitions), targets, _ = prep_fn(mb, model.vocab)\n",
        "            root_logits, _ = model((x, transitions))  # forward pass\n",
        "\n",
        "            predictions = root_logits.argmax(dim=-1)  # [B]\n",
        "            targets = targets.view(-1)               # [B]\n",
        "\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    accuracy = correct / float(total)\n",
        "    return correct, total, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh9RbhGwaiLg"
      },
      "source": [
        "## Special `prepare` function for Tree-LSTM\n",
        "\n",
        "We need yet another `prepare` function. For our implementation, sentences need to be *reversed*. We will do that here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "KuN8YeQdqZDg"
      },
      "outputs": [],
      "source": [
        "from nltk import Tree\n",
        "\n",
        "def extract_subtree_labels(tree: Tree):\n",
        "    \"\"\"\n",
        "    Extracts the label of each subtree in post-order.\n",
        "    This ensures the order matches REDUCE operations in a transition-based TreeLSTM.\n",
        "\n",
        "    Args:\n",
        "        tree (nltk.Tree): The parse tree with labels at each node.\n",
        "\n",
        "    Returns:\n",
        "        labels (List[int]): List of labels for each subtree node.\n",
        "                            Leaves are usually ignored or can be included.\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "\n",
        "    def traverse(t):\n",
        "        # If the node has children that are trees, traverse them first\n",
        "        for child in t:\n",
        "            if isinstance(child, Tree):\n",
        "                traverse(child)\n",
        "\n",
        "        # Append current node label\n",
        "        labels.append(t.label())\n",
        "\n",
        "    traverse(tree)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "DiqH-_2xdm9H"
      },
      "outputs": [],
      "source": [
        "def prepare_treelstm_minibatch(mb, vocab):\n",
        "    \"\"\"\n",
        "    Returns sentences reversed (last word first), transitions, and optionally subtree labels.\n",
        "    \"\"\"\n",
        "    batch_size = len(mb)\n",
        "    maxlen = max([len(ex.tokens) for ex in mb])\n",
        "\n",
        "    # vocab returns 0 if the word is not there\n",
        "    # NOTE: reversed sequence!\n",
        "    x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen)[::-1] for ex in mb]\n",
        "    x = torch.LongTensor(x).to(device)\n",
        "\n",
        "    # sentence-level labels\n",
        "    y = [ex.label for ex in mb]\n",
        "    y = torch.LongTensor(y).to(device)\n",
        "\n",
        "    # transitions\n",
        "    maxlen_t = max([len(ex.transitions) for ex in mb])\n",
        "    transitions = [pad(ex.transitions, maxlen_t, pad_value=2) for ex in mb]\n",
        "    transitions = np.array(transitions)\n",
        "    transitions = transitions.T  # time-major\n",
        "\n",
        "    # --- subtree labels ---\n",
        "    # extract labels for each node in post-order, matching transitions\n",
        "    subtree_labels = [extract_subtree_labels(ex.tree) for ex in mb]\n",
        "\n",
        "    return (x, transitions), y, subtree_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMUsrlL9ayVe"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAFhGaAN7hoL"
      },
      "source": [
        "### **Using Subtrees in TreeLSTM Training: Two Approaches**\n",
        "\n",
        "When training a TreeLSTM for tasks like sentiment analysis, you have the option of leveraging **subtree nodes** (internal nodes in the parse tree) in addition to the root node. This can enrich supervision and improve representation learning. There are two main approaches:\n",
        "\n",
        "---\n",
        "\n",
        "## **1️⃣ Compute all node embeddings once per sentence and include all subtree losses in the batch**\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "* Forward pass: Compute hidden states for all nodes in the tree during a **single pass** of the TreeLSTM.\n",
        "* Loss computation: Apply the classifier to each subtree node and sum or average their losses with the root node loss.\n",
        "* Batch: Each batch contains complete sentences, with all subtrees of each sentence included in order.\n",
        "\n",
        "**Intuition / Justification:**\n",
        "\n",
        "* **Efficiency**: Every node’s embedding is computed exactly once. No redundant forward passes, which is important for large trees or long sentences.\n",
        "* **Hierarchical learning**: Supervising internal nodes encourages the model to learn meaningful representations at all levels of the tree, not just at the root.\n",
        "* **Dataset enrichment**: Each sentence now contributes multiple supervised examples (the subtrees), effectively increasing the amount of training data without additional data collection.\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "1. ✅ **Computationally efficient** — no repeated computation of node embeddings.\n",
        "2. ✅ **Captures hierarchy** — supervision on internal nodes helps the model propagate useful features up the tree.\n",
        "3. ✅ **Rich signal per sentence** — even a single sentence gives multiple examples for the model to learn from.\n",
        "\n",
        "**Cons / Caveats:**\n",
        "\n",
        "1. ⚠️ **Limited stochasticity**: Since all nodes from a sentence are in the same batch, the examples are highly correlated. This can reduce the benefits of stochastic gradient descent.\n",
        "2. ⚠️ **Long sentences dominate**: Sentences with more nodes contribute more to the loss, potentially biasing the model toward long trees.\n",
        "3. ⚠️ **Memory usage**: Storing logits for all nodes may become expensive for very large trees.\n",
        "\n",
        "**Best use-case:** When efficiency is critical and your batch size can include multiple sentences, this approach is widely used in TreeLSTM papers (e.g., Tai et al., 2015).\n",
        "\n",
        "---\n",
        "\n",
        "## **2️⃣ Flatten subtree nodes and treat each as an independent training example**\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "* Preprocess: Extract all node embeddings for all sentences. Flatten them into a list of `(node_hidden, node_label)` pairs.\n",
        "* Shuffle: Mix all nodes from all sentences, so batches contain nodes from different sentences.\n",
        "* Loss computation: Apply the classifier to each node independently.\n",
        "\n",
        "**Intuition / Justification:**\n",
        "\n",
        "* **Better shuffling**: Nodes from different sentences are mixed, improving stochasticity and reducing correlation within batches.\n",
        "* **Balanced influence**: Each node contributes equally to the loss, avoiding dominance by long sentences.\n",
        "* **Standard supervised training**: Easier to integrate with standard mini-batch pipelines.\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "1. ✅ **Good stochasticity** — batches contain a diverse set of nodes.\n",
        "2. ✅ **Balanced supervision** — long sentences do not dominate training.\n",
        "3. ✅ **Flexible batching** — node-level batches can be of any size, independent of sentence length.\n",
        "\n",
        "**Cons / Caveats:**\n",
        "\n",
        "1. ⚠️ **Less efficient if embeddings not cached** — computing hidden states for each node separately could be expensive unless you precompute them.\n",
        "2. ⚠️ **Loss of tree context in batches** — if you process nodes independently, some hierarchical relationships may be less emphasized, though the embeddings themselves still contain tree information.\n",
        "3. ⚠️ **Implementation complexity** — requires flattening and bookkeeping of nodes and labels.\n",
        "\n",
        "**Best use-case:** When you want maximum stochasticity and fair contribution of each node, or when batching is constrained by GPU memory.\n",
        "\n",
        "---\n",
        "\n",
        "### **Intuitive Summary**\n",
        "\n",
        "| Aspect                          | Compute Once Per Sentence                    | Flatten & Shuffle Nodes                               |\n",
        "| ------------------------------- | -------------------------------------------- | ----------------------------------------------------- |\n",
        "| **Efficiency**                  | Very high — single forward pass per sentence | Medium — may require separate passes or caching       |\n",
        "| **Stochasticity**               | Low — nodes of same sentence in same batch   | High — nodes shuffled across sentences                |\n",
        "| **Hierarchical supervision**    | High — nodes maintain tree context           | Medium — context embedded but not sequential in batch |\n",
        "| **Bias towards long sentences** | High — more nodes = more weight              | Low — each node contributes equally                   |\n",
        "| **Implementation complexity**   | Moderate — straightforward with TreeLSTM     | Higher — need flattening & shuffling                  |\n",
        "| **Memory usage**                | High if many nodes per batch                 | Moderate — can control batch size independently       |\n",
        "\n",
        "**Key takeaway:**\n",
        "\n",
        "* **Your current approach** (compute node embeddings once, sum losses) is **standard practice**, effective, and computationally efficient.\n",
        "* **Optional improvement**: Shuffle nodes across sentences per batch to improve SGD dynamics and reduce bias from long sentences, while still computing embeddings once.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TreeLSTMClassifier(\n",
            "  (embed): Embedding(18922, 300, padding_idx=1)\n",
            "  (treelstm): TreeLSTM(\n",
            "    (reduce): TreeLSTMCell(300, 150)\n",
            "    (proj_x): Linear(in_features=300, out_features=150, bias=True)\n",
            "    (proj_x_gate): Linear(in_features=300, out_features=150, bias=True)\n",
            "    (buffers_dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (output_layer): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=150, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "embed.weight             [18922, 300] requires_grad=True\n",
            "treelstm.reduce.reduce_layer.weight [750, 300]   requires_grad=True\n",
            "treelstm.reduce.reduce_layer.bias [750]        requires_grad=True\n",
            "treelstm.proj_x.weight   [150, 300]   requires_grad=True\n",
            "treelstm.proj_x.bias     [150]        requires_grad=True\n",
            "treelstm.proj_x_gate.weight [150, 300]   requires_grad=True\n",
            "treelstm.proj_x_gate.bias [150]        requires_grad=True\n",
            "output_layer.1.weight    [5, 150]     requires_grad=True\n",
            "output_layer.1.bias      [5]          requires_grad=True\n",
            "\n",
            "Total number of parameters: 5993405\n",
            "\n",
            "TreeLSTMClassifier(\n",
            "  (embed): Embedding(18922, 300, padding_idx=1)\n",
            "  (treelstm): TreeLSTM(\n",
            "    (reduce): TreeLSTMCell(300, 150)\n",
            "    (proj_x): Linear(in_features=300, out_features=150, bias=True)\n",
            "    (proj_x_gate): Linear(in_features=300, out_features=150, bias=True)\n",
            "    (buffers_dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (output_layer): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=150, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "embed.weight             [18922, 300] requires_grad=False\n",
            "treelstm.reduce.reduce_layer.weight [750, 300]   requires_grad=True\n",
            "treelstm.reduce.reduce_layer.bias [750]        requires_grad=True\n",
            "treelstm.proj_x.weight   [150, 300]   requires_grad=True\n",
            "treelstm.proj_x.bias     [150]        requires_grad=True\n",
            "treelstm.proj_x_gate.weight [150, 300]   requires_grad=True\n",
            "treelstm.proj_x_gate.bias [150]        requires_grad=True\n",
            "output_layer.1.weight    [5, 150]     requires_grad=True\n",
            "output_layer.1.bias      [5]          requires_grad=True\n",
            "\n",
            "Total number of parameters: 5993405\n",
            "\n",
            "Shuffling training data\n",
            "Iter 250: loss=694.5882, time=12.80s\n",
            "Iter 250: dev acc=0.2443\n",
            "New highscore!\n",
            "Shuffling training data\n",
            "Iter 500: loss=633.3839, time=25.31s\n",
            "Iter 500: dev acc=0.2643\n",
            "New highscore!\n",
            "Shuffling training data\n",
            "Iter 750: loss=626.0882, time=38.19s\n",
            "Iter 750: dev acc=0.2698\n",
            "New highscore!\n",
            "Iter 1000: loss=622.6809, time=50.48s\n",
            "Iter 1000: dev acc=0.2734\n",
            "New highscore!\n",
            "Shuffling training data\n",
            "Iter 1250: loss=614.3834, time=63.01s\n",
            "Iter 1250: dev acc=0.3252\n",
            "New highscore!\n",
            "Shuffling training data\n",
            "Iter 1500: loss=603.2291, time=75.78s\n",
            "Iter 1500: dev acc=0.3579\n",
            "New highscore!\n",
            "Shuffling training data\n",
            "Iter 1750: loss=579.3321, time=88.71s\n",
            "Iter 1750: dev acc=0.3769\n",
            "New highscore!\n",
            "Iter 2000: loss=570.3825, time=102.14s\n",
            "Iter 2000: dev acc=0.3978\n",
            "New highscore!\n",
            "Shuffling training data\n",
            "Iter 2250: loss=564.0853, time=114.92s\n",
            "Iter 2250: dev acc=0.4005\n",
            "New highscore!\n",
            "Shuffling training data\n",
            "Iter 2500: loss=560.4701, time=127.67s\n",
            "Iter 2500: dev acc=0.3815\n",
            "Shuffling training data\n",
            "Iter 2750: loss=552.5524, time=140.20s\n",
            "Iter 2750: dev acc=0.3797\n",
            "Iter 3000: loss=550.0276, time=152.93s\n",
            "Iter 3000: dev acc=0.4051\n",
            "New highscore!\n",
            "Shuffling training data\n",
            "Iter 3250: loss=549.5663, time=165.05s\n",
            "Iter 3250: dev acc=0.4060\n",
            "New highscore!\n",
            "Shuffling training data\n",
            "Iter 3500: loss=544.1766, time=177.45s\n",
            "Iter 3500: dev acc=0.4269\n",
            "New highscore!\n",
            "Iter 3750: loss=546.4719, time=189.72s\n",
            "Iter 3750: dev acc=0.4078\n",
            "Shuffling training data\n",
            "Iter 4000: loss=540.7105, time=202.19s\n",
            "Iter 4000: dev acc=0.4351\n",
            "New highscore!\n",
            "Shuffling training data\n",
            "Iter 4250: loss=538.5431, time=215.10s\n",
            "Iter 4250: dev acc=0.4178\n",
            "Shuffling training data\n",
            "Iter 4500: loss=539.4891, time=229.53s\n",
            "Iter 4500: dev acc=0.4296\n",
            "Iter 4750: loss=536.3980, time=243.96s\n",
            "Iter 4750: dev acc=0.4296\n",
            "Shuffling training data\n",
            "Iter 5000: loss=533.4686, time=257.71s\n",
            "Iter 5000: dev acc=0.4351\n",
            "Shuffling training data\n",
            "Iter 5250: loss=535.0886, time=269.83s\n",
            "Iter 5250: dev acc=0.4269\n",
            "Shuffling training data\n",
            "Iter 5500: loss=531.8840, time=282.49s\n",
            "Iter 5500: dev acc=0.4423\n",
            "New highscore!\n",
            "Iter 5750: loss=529.7537, time=295.00s\n",
            "Iter 5750: dev acc=0.4342\n",
            "Shuffling training data\n",
            "Iter 6000: loss=527.5702, time=307.33s\n",
            "Iter 6000: dev acc=0.4260\n",
            "Shuffling training data\n",
            "Iter 6250: loss=529.8628, time=320.18s\n",
            "Iter 6250: dev acc=0.4214\n",
            "Shuffling training data\n",
            "Iter 6500: loss=527.1165, time=332.79s\n",
            "Iter 6500: dev acc=0.4114\n",
            "Iter 6750: loss=523.6825, time=346.01s\n",
            "Iter 6750: dev acc=0.4332\n",
            "Shuffling training data\n",
            "Iter 7000: loss=524.0231, time=358.93s\n",
            "Iter 7000: dev acc=0.4432\n",
            "New highscore!\n",
            "Shuffling training data\n",
            "Iter 7250: loss=525.8884, time=372.04s\n",
            "Iter 7250: dev acc=0.4205\n",
            "Iter 7500: loss=522.4822, time=384.99s\n",
            "Iter 7500: dev acc=0.4169\n",
            "Shuffling training data\n",
            "Iter 7750: loss=522.4043, time=398.15s\n",
            "Iter 7750: dev acc=0.4223\n",
            "Shuffling training data\n",
            "Iter 8000: loss=522.8660, time=411.86s\n",
            "Iter 8000: dev acc=0.4242\n",
            "Shuffling training data\n",
            "Iter 8250: loss=518.1368, time=424.04s\n",
            "Iter 8250: dev acc=0.4314\n",
            "Iter 8500: loss=521.3789, time=437.28s\n",
            "Iter 8500: dev acc=0.4269\n",
            "Shuffling training data\n",
            "Iter 8750: loss=514.5839, time=449.48s\n",
            "Iter 8750: dev acc=0.4387\n",
            "Shuffling training data\n",
            "Iter 9000: loss=514.1492, time=461.70s\n",
            "Iter 9000: dev acc=0.4187\n",
            "Shuffling training data\n",
            "Iter 9250: loss=519.3809, time=473.94s\n",
            "Iter 9250: dev acc=0.4187\n",
            "Iter 9500: loss=513.2802, time=486.57s\n",
            "Iter 9500: dev acc=0.4242\n",
            "Shuffling training data\n",
            "Iter 9750: loss=511.2815, time=498.66s\n",
            "Iter 9750: dev acc=0.4432\n",
            "Shuffling training data\n",
            "Iter 10000: loss=512.9135, time=511.45s\n",
            "Iter 10000: dev acc=0.4460\n",
            "New highscore!\n",
            "Iter 10250: loss=511.5911, time=524.47s\n",
            "Iter 10250: dev acc=0.4432\n",
            "Shuffling training data\n",
            "Iter 10500: loss=510.6402, time=538.26s\n",
            "Iter 10500: dev acc=0.4469\n",
            "New highscore!\n",
            "Shuffling training data\n",
            "Iter 10750: loss=505.4846, time=551.58s\n",
            "Iter 10750: dev acc=0.4223\n",
            "Shuffling training data\n",
            "Iter 11000: loss=509.0172, time=564.26s\n",
            "Iter 11000: dev acc=0.4342\n",
            "Iter 11250: loss=509.3482, time=577.32s\n",
            "Iter 11250: dev acc=0.4296\n",
            "Shuffling training data\n",
            "Iter 11500: loss=506.8374, time=590.30s\n",
            "Iter 11500: dev acc=0.4396\n",
            "Shuffling training data\n",
            "Iter 11750: loss=503.5075, time=603.92s\n",
            "Iter 11750: dev acc=0.4242\n",
            "Shuffling training data\n",
            "Iter 12000: loss=504.3758, time=617.37s\n",
            "Iter 12000: dev acc=0.4378\n",
            "Iter 12250: loss=501.6716, time=630.75s\n",
            "Iter 12250: dev acc=0.4142\n",
            "Shuffling training data\n",
            "Iter 12500: loss=502.5948, time=643.22s\n",
            "Iter 12500: dev acc=0.4514\n",
            "New highscore!\n",
            "Shuffling training data\n",
            "Iter 12750: loss=503.6846, time=656.07s\n",
            "Iter 12750: dev acc=0.4414\n",
            "Shuffling training data\n",
            "Iter 13000: loss=499.8817, time=668.74s\n",
            "Iter 13000: dev acc=0.4514\n",
            "Iter 13250: loss=498.3170, time=681.16s\n",
            "Iter 13250: dev acc=0.4278\n",
            "Shuffling training data\n",
            "Iter 13500: loss=498.4994, time=694.91s\n",
            "Iter 13500: dev acc=0.4478\n",
            "Shuffling training data\n",
            "Iter 13750: loss=498.1473, time=708.06s\n",
            "Iter 13750: dev acc=0.4369\n",
            "Iter 14000: loss=495.9651, time=720.02s\n",
            "Iter 14000: dev acc=0.4251\n",
            "Shuffling training data\n",
            "Iter 14250: loss=492.1334, time=732.53s\n",
            "Iter 14250: dev acc=0.4378\n",
            "Shuffling training data\n",
            "Iter 14500: loss=495.9622, time=744.62s\n",
            "Iter 14500: dev acc=0.4369\n",
            "Shuffling training data\n",
            "Iter 14750: loss=493.5082, time=756.99s\n",
            "Iter 14750: dev acc=0.4396\n",
            "Iter 15000: loss=490.3775, time=768.95s\n",
            "Iter 15000: dev acc=0.4487\n",
            "Done training\n",
            "Best model iter 12500: train acc=0.5350, dev acc=0.4514, test acc=0.4570\n"
          ]
        }
      ],
      "source": [
        "# Now let's train the Tree LSTM!\n",
        "\n",
        "tree_model = TreeLSTMClassifier(\n",
        "    len(word2vec_vocab.w2i), 300, 150, len(t2i), word2vec_vocab, \"N-ary\")\n",
        "\n",
        "print(tree_model)\n",
        "print_parameters(tree_model)\n",
        "torch.manual_seed(42)\n",
        "  # When running on the CuDNN backend two further options must be set for reproducibility\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False \n",
        "\n",
        "with torch.no_grad():\n",
        "  tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "  tree_model.embed.weight.requires_grad = False\n",
        "\n",
        "def do_train(model):\n",
        "\n",
        "  print(model)\n",
        "  print_parameters(model)\n",
        "  model = model.to(device)\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "\n",
        "  return train_model_w_subtree(\n",
        "      model, optimizer, num_iterations=15000,\n",
        "      print_every=250, eval_every=250,\n",
        "      prep_fn=prepare_treelstm_minibatch,\n",
        "      eval_fn=evaluate_root,\n",
        "      batch_fn=get_minibatch,\n",
        "      batch_size=25, eval_batch_size=25)\n",
        "\n",
        "results = do_train(tree_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zSo0NLn44iP",
        "outputId": "782b7736-714e-4c56-85bc-1486f9c298b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "([694.5881867408752, 633.3838977813721, 626.0881540775299, 622.6809206008911, 614.3834068775177, 603.2291250228882, 579.3321485519409, 570.3825399875641, 564.0853068828583, 560.4701195955276, 552.5523554086685, 550.0276124477386, 549.5663237571716, 544.176612496376, 546.4719051122665, 540.7104653120041, 538.5430629253387, 539.4890648126602, 536.3980355262756, 533.4685571193695, 535.0885585546494, 531.8840235471725, 529.7537115812302, 527.5702183246613, 529.8627941608429, 527.1165410280228, 523.6825295686722, 524.0230840444565, 525.8883873224258, 522.4821627140045, 522.4042580127716, 522.8659927845001, 518.1368482112885, 521.3788615465164, 514.5838613510132, 514.1491755247116, 519.3808634281158, 513.2801787853241, 511.2815372943878, 512.9135411977768, 511.59110367298126, 510.64020574092865, 505.4846296310425, 509.0172210931778, 509.3482121229172, 506.8374319076538, 503.5075422525406, 504.3757643699646, 501.671630859375, 502.5948334932327, 503.68462681770325, 499.881658911705, 498.3169856071472, 498.49944508075714, 498.1473118066788, 495.9651435613632, 492.1334116458893, 495.96217381954193, 493.5082029104233, 490.3775168657303], [0.24432334241598547, 0.26430517711171664, 0.26975476839237056, 0.27338782924613986, 0.32515894641235243, 0.3578564940962761, 0.37693006357856496, 0.3978201634877384, 0.40054495912806537, 0.3814713896457766, 0.3796548592188919, 0.405086285195277, 0.40599455040871935, 0.4268846503178928, 0.407811080835604, 0.43505903723887374, 0.4178019981834696, 0.4296094459582198, 0.4296094459582198, 0.43505903723887374, 0.4268846503178928, 0.44232515894641233, 0.43415077202543145, 0.4259763851044505, 0.4214350590372389, 0.4114441416893733, 0.4332425068119891, 0.44323342415985467, 0.42052679382379654, 0.41689373297002724, 0.4223433242506812, 0.42415985467756584, 0.43142597638510444, 0.4268846503178928, 0.43869209809264303, 0.4187102633969119, 0.4187102633969119, 0.42415985467756584, 0.44323342415985467, 0.44595821980018163, 0.44323342415985467, 0.44686648501362397, 0.4223433242506812, 0.43415077202543145, 0.4296094459582198, 0.4396003633060854, 0.42415985467756584, 0.43778383287920075, 0.4141689373297003, 0.4514078110808356, 0.44141689373297005, 0.4514078110808356, 0.42779291553133514, 0.4477747502270663, 0.4368755676657584, 0.4250681198910082, 0.43778383287920075, 0.4368755676657584, 0.4396003633060854, 0.44868301544050865])\n"
          ]
        }
      ],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save results as json file\n",
        "import json\n",
        "with open(\"treelstm_with_supervision_results.json\", \"w\") as outfile:\n",
        "    json.dump({\n",
        "        \"losses\": results[0],\n",
        "        \"accuracies\": results[1]\n",
        "    }, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plor the accuracies and losses in a nice way\n",
        "import matplotlib.pyplot as plt\n",
        "with open(\"treelstm_with_supervision_results.json\", \"r\") as infile:\n",
        "    results = json.load(infile)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(results[\"losses\"])\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Iterations (x250)\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(results[\"accuracies\"])\n",
        "plt.title(\"Dev Accuracy\")\n",
        "plt.xlabel(\"Evaluations (x250)\")\n",
        "plt.ylim(0,1)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
