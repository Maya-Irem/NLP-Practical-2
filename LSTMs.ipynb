{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "filename = \"trainDevTestTrees_PTB.zip\"\n",
    "# --- Unzipping the file ---\n",
    "print(\"Extracting files...\")\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    # Extracts all contents into the current working directory\n",
    "    zip_ref.extractall(os.getcwd()) \n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function reads in a textfile and fixes an issue with \"\\\\\"\n",
    "def filereader(path):\n",
    "  with open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "      yield line.strip().replace(\"\\\\\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Tree\n",
    "from nltk.treeprettyprinter import TreePrettyPrinter\n",
    "\n",
    "s = next(filereader(\"trees/dev.txt\"))\n",
    "print(s)\n",
    "# We can use NLTK to better visualise the tree structure of the sentence\n",
    "from nltk import Tree\n",
    "from nltk.treeprettyprinter import TreePrettyPrinter\n",
    "tree = Tree.fromstring(s)\n",
    "print(TreePrettyPrinter(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first make a function that extracts the tokens (the leaves).\n",
    "\n",
    "def tokens_from_treestring(s):\n",
    "  \"\"\"extract the tokens from a sentiment tree\"\"\"\n",
    "  return re.sub(r\"\\([0-9] |\\)\", \"\", s).split()\n",
    "\n",
    "# let's try it on our example tree\n",
    "tokens = tokens_from_treestring(s)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also need the following function, but you can ignore this for now.\n",
    "# It is explained later on.\n",
    "\n",
    "SHIFT = 0\n",
    "REDUCE = 1\n",
    "\n",
    "\n",
    "def transitions_from_treestring(s):\n",
    "  s = re.sub(\"\\([0-5] ([^)]+)\\)\", \"0\", s)\n",
    "  s = re.sub(\"\\)\", \" )\", s)\n",
    "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
    "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
    "  s = re.sub(\"\\)\", \"1\", s)\n",
    "  return list(map(int, s.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's first see how large our data sets are.\n",
    "for path in (\"trees/train.txt\", \"trees/dev.txt\", \"trees/test.txt\"):\n",
    "  print(\"{:16s} {:4d}\".format(path, sum(1 for _ in filereader(path))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from nltk import Tree\n",
    "\n",
    "# A simple way to define a class is using namedtuple.\n",
    "Example = namedtuple(\"Example\", [\"tokens\", \"tree\", \"label\", \"transitions\"])\n",
    "\n",
    "\n",
    "def examplereader(path, lower=False):\n",
    "  \"\"\"Returns all examples in a file one by one.\"\"\"\n",
    "  for line in filereader(path):\n",
    "    line = line.lower() if lower else line\n",
    "    tokens = tokens_from_treestring(line)\n",
    "    tree = Tree.fromstring(line)  # use NLTK's Tree\n",
    "    label = int(line[1])\n",
    "    trans = transitions_from_treestring(line)\n",
    "    yield Example(tokens=tokens, tree=tree, label=label, transitions=trans)\n",
    "\n",
    "\n",
    "# Let's load the data into memory.\n",
    "LOWER = False  # we will keep the original casing\n",
    "train_data = list(examplereader(\"trees/train.txt\", lower=LOWER))\n",
    "dev_data = list(examplereader(\"trees/dev.txt\", lower=LOWER))\n",
    "test_data = list(examplereader(\"trees/test.txt\", lower=LOWER))\n",
    "\n",
    "print(\"train\", len(train_data))\n",
    "print(\"dev\", len(dev_data))\n",
    "print(\"test\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first define a class that can map a word to an ID (w2i)\n",
    "# and back (i2w).\n",
    "\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "  \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
    "  def __repr__(self):\n",
    "    return '%s(%r)' % (self.__class__.__name__,\n",
    "                      OrderedDict(self))\n",
    "  def __reduce__(self):\n",
    "    return self.__class__, (OrderedDict(self),)\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "  \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self.freqs = OrderedCounter()\n",
    "    self.w2i = {}\n",
    "    self.i2w = []\n",
    "\n",
    "  def count_token(self, t):\n",
    "    self.freqs[t] += 1\n",
    "\n",
    "  def add_token(self, t):\n",
    "    self.w2i[t] = len(self.w2i)\n",
    "    self.i2w.append(t)\n",
    "\n",
    "  def build(self, min_freq=0):\n",
    "    '''\n",
    "    min_freq: minimum number of occurrences for a word to be included\n",
    "              in the vocabulary\n",
    "    '''\n",
    "    self.add_token(\"<unk>\")  # reserve 0 for <unk> (unknown words)\n",
    "    self.add_token(\"<pad>\")  # reserve 1 for <pad> (discussed later)\n",
    "\n",
    "    tok_freq = list(self.freqs.items())\n",
    "    tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "    for tok, freq in tok_freq:\n",
    "      if freq >= min_freq:\n",
    "        self.add_token(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This process should be deterministic and should have the same result\n",
    "# if run multiple times on the same data set.\n",
    "\n",
    "v = Vocabulary()\n",
    "for data_set in (train_data,):\n",
    "  for ex in data_set:\n",
    "    for token in ex.tokens:\n",
    "      v.count_token(token)\n",
    "\n",
    "v.build()\n",
    "print(\"Vocabulary size:\", len(v.w2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's map the sentiment labels 0-4 to a more readable form\n",
    "i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
    "print(i2t)\n",
    "print(i2t[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's also create the opposite mapping.\n",
    "# We won't use a Vocabulary for this (although we could), since the labels\n",
    "# are already numeric.\n",
    "t2i = OrderedDict({p : i for p, i in zip(i2t, range(len(i2t)))})\n",
    "print(t2i)\n",
    "print(t2i['very positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_example(example, vocab):\n",
    "  \"\"\"\n",
    "  Map tokens to their IDs for a single example\n",
    "  \"\"\"\n",
    "\n",
    "  # vocab returns 0 if the word is not there (i2w[0] = <unk>)\n",
    "  x = [vocab.w2i.get(t, 0) for t in example.tokens]\n",
    "\n",
    "  x = torch.LongTensor([x])\n",
    "  x = x.to(device)\n",
    "\n",
    "  y = torch.LongTensor([example.label])\n",
    "  y = y.to(device)\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
    "  \"\"\"Accuracy of a model on given data set.\"\"\"\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  model.eval()  # disable dropout (explained later)\n",
    "\n",
    "  for example in data:\n",
    "\n",
    "    # convert the example input and label to PyTorch tensors\n",
    "    x, target = prep_fn(example, model.vocab)\n",
    "\n",
    "    # forward pass without backpropagation (no_grad)\n",
    "    # get the output from the neural network for input x\n",
    "    with torch.no_grad():\n",
    "      logits = model(x)\n",
    "\n",
    "    # get the prediction\n",
    "    prediction = logits.argmax(dim=-1)\n",
    "\n",
    "    # add the number of correct predictions to the total correct\n",
    "    correct += (prediction == target).sum().item()\n",
    "    total += 1\n",
    "\n",
    "  return correct, total, correct / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(data, shuffle=True, **kwargs):\n",
    "  \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
    "  if shuffle:\n",
    "    print(\"Shuffling training data\")\n",
    "    random.shuffle(data)  # shuffle training data each epoch\n",
    "  for example in data:\n",
    "    yield example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, optimizer, num_iterations=10000,\n",
    "                print_every=1000, eval_every=1000,\n",
    "                batch_fn=get_examples,\n",
    "                prep_fn=prepare_example,\n",
    "                eval_fn=simple_evaluate,\n",
    "                batch_size=1, eval_batch_size=None, seed=17):\n",
    "  \"\"\"Train a model.\"\"\"\n",
    "  iter_i = 0\n",
    "  train_loss = 0.\n",
    "  print_num = 0\n",
    "  start = time.time()\n",
    "  criterion = nn.CrossEntropyLoss() # loss function\n",
    "  best_eval = 0.\n",
    "  best_iter = 0\n",
    "\n",
    "  # store train loss and validation accuracy during training\n",
    "  # so we can plot them afterwards\n",
    "  losses = []\n",
    "  accuracies = []\n",
    "\n",
    "  if eval_batch_size is None:\n",
    "    eval_batch_size = batch_size\n",
    "\n",
    "  while True:  # when we run out of examples, shuffle and continue\n",
    "    for batch in tqdm(batch_fn(train_data, batch_size=batch_size)): \n",
    "\n",
    "      # forward pass\n",
    "      model.train()\n",
    "      x, targets = prep_fn(batch, model.vocab)\n",
    "      logits = model(x)\n",
    "\n",
    "      B = targets.size(0)  # later we will use B examples per update\n",
    "\n",
    "      # compute cross-entropy loss (our criterion)\n",
    "      # note that the cross entropy loss function computes the softmax for us\n",
    "      loss = criterion(logits.view([B, -1]), targets.view(-1))\n",
    "      train_loss += loss.item()\n",
    "\n",
    "      # backward pass (tip: check the Introduction to PyTorch notebook)\n",
    "\n",
    "      # ========== MY CODE ==============\n",
    "      # erase previous gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # compute gradients\n",
    "      loss.backward()\n",
    "\n",
    "      # update weights - take a small step in the opposite dir of the gradient\n",
    "      optimizer.step()\n",
    "\n",
    "      print_num += 1\n",
    "      iter_i += 1\n",
    "\n",
    "      # print info\n",
    "      if iter_i % print_every == 0:\n",
    "        print(\"Iter %r: loss=%.4f, time=%.2fs\" %\n",
    "              (iter_i, train_loss, time.time()-start))\n",
    "        losses.append(train_loss)\n",
    "        print_num = 0\n",
    "        train_loss = 0.\n",
    "\n",
    "      # evaluate\n",
    "      if iter_i % eval_every == 0:\n",
    "        _, _, accuracy = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                                 batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"iter %r: dev acc=%.4f\" % (iter_i, accuracy))\n",
    "\n",
    "        # save best model parameters\n",
    "        if accuracy > best_eval:\n",
    "          print(\"new highscore\")\n",
    "          best_eval = accuracy\n",
    "          best_iter = iter_i\n",
    "          path = \"{}_{}.pt\".format(model.__class__.__name__, seed)\n",
    "          ckpt = {\n",
    "              \"state_dict\": model.state_dict(),\n",
    "              \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "              \"best_eval\": best_eval,\n",
    "              \"best_iter\": best_iter\n",
    "          }\n",
    "          torch.save(ckpt, path)\n",
    "\n",
    "      # done training\n",
    "      if iter_i == num_iterations:\n",
    "        print(\"Done training\")\n",
    "\n",
    "        # evaluate on train, dev, and test with best model\n",
    "        print(\"Loading best model\")\n",
    "        path = \"{}_{}.pt\".format(model.__class__.__name__, seed)\n",
    "        ckpt = torch.load(path)\n",
    "        model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "        _, _, train_acc = eval_fn(\n",
    "            model, train_data, batch_size=eval_batch_size,\n",
    "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "        _, _, dev_acc = eval_fn(\n",
    "            model, dev_data, batch_size=eval_batch_size,\n",
    "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "        _, _, test_acc = eval_fn(\n",
    "            model, test_data, batch_size=eval_batch_size,\n",
    "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "\n",
    "        print(\"best model iter {:d}: \"\n",
    "              \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "                  best_iter, train_acc, dev_acc, test_acc))\n",
    "\n",
    "        return losses, accuracies, best_iter, train_acc, dev_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import random\n",
    "\n",
    "def train_model_w_seed(model, optimizer_class, optimizer_params,\n",
    "                       train_data, dev_data, test_data,\n",
    "                       num_iterations=10000,\n",
    "                       print_every=1000,\n",
    "                       eval_every=1000,\n",
    "                       batch_fn=get_examples,\n",
    "                       prep_fn=prepare_example,\n",
    "                       eval_fn=simple_evaluate,\n",
    "                       batch_size=1,\n",
    "                       eval_batch_size=None):\n",
    "\n",
    "    test_accs = []\n",
    "    best_iters = []\n",
    "    train_accs = []\n",
    "    dev_accs = []\n",
    "\n",
    "    list_of_accuracies = []\n",
    "    list_of_losses = []\n",
    "\n",
    "    # create 3 copies of the model\n",
    "    models = [copy.deepcopy(model) for _ in range(3)]\n",
    "    seeds = [17, 42, 2025]\n",
    "\n",
    "    for seed, model_copy in zip(seeds, models):\n",
    "        # Set seeds for reproducibility\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        # Create a fresh optimizer for each model\n",
    "        # optimizer = optimizer_class(model_copy.parameters(), **optimizer_params)\n",
    "        # Changed this to optimize only parameters that require gradients\n",
    "        optimizer = optimizer_class(\n",
    "            filter(lambda p: p.requires_grad, model_copy.parameters()),\n",
    "            **optimizer_params\n",
    "        )\n",
    "\n",
    "        # Call your inner training function\n",
    "        # Make sure your train_model function returns:\n",
    "        # losses, accuracies, best_iter, train_acc, dev_acc, test_acc\n",
    "        losses, accuracies, best_iter, train_acc, dev_acc, test_acc = train_model(\n",
    "            model_copy, optimizer,\n",
    "            num_iterations=num_iterations,\n",
    "            print_every=print_every,\n",
    "            eval_every=eval_every,\n",
    "            batch_fn=batch_fn,\n",
    "            prep_fn=prep_fn,\n",
    "            eval_fn=eval_fn,\n",
    "            batch_size=batch_size,\n",
    "            eval_batch_size=eval_batch_size,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        list_of_accuracies.append(accuracies)\n",
    "        list_of_losses.append(losses)\n",
    "        best_iters.append(best_iter)\n",
    "        train_accs.append(train_acc)\n",
    "        dev_accs.append(dev_acc)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "    # Compute mean and std\n",
    "    mean_train_acc = np.mean(train_accs)\n",
    "    mean_dev_acc = np.mean(dev_accs)\n",
    "    mean_test_acc = np.mean(test_accs)\n",
    "    mean_best_iter = np.mean(best_iters)\n",
    "\n",
    "    std_train_acc = np.std(train_accs)\n",
    "    std_dev_acc = np.std(dev_accs)\n",
    "    std_test_acc = np.std(test_accs)\n",
    "    std_best_iter = np.std(best_iters)\n",
    "\n",
    "    # Create JSON\n",
    "    json_data = {\n",
    "        'train_accs_mean': mean_train_acc,\n",
    "        'dev_accs_mean': mean_dev_acc,\n",
    "        'test_accs_mean': mean_test_acc,\n",
    "\n",
    "        'train_accs_std': std_train_acc,\n",
    "        'dev_accs_std': std_dev_acc,\n",
    "        'test_accs_std': std_test_acc,\n",
    "\n",
    "        'train_accs': train_accs,\n",
    "        'dev_accs': dev_accs,\n",
    "        'test_accs': test_accs,\n",
    "        'best_iters': best_iters,\n",
    "\n",
    "        'list_of_losses': list_of_losses,\n",
    "        'list_of_accuracies': list_of_accuracies\n",
    "    }\n",
    "\n",
    "    # Save JSON file once\n",
    "    filename = f\"{model.__class__.__name__}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the validation accuracies over time\n",
    "def plot_accuracies(list_of_accuracies, model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, accuracies in enumerate(list_of_accuracies):\n",
    "        plt.plot(range(len(accuracies)), accuracies, label=f'Run {i+1}')\n",
    "    plt.title(f'Validation Accuracies over Time for {model_name}')\n",
    "    plt.xlabel('Evaluation Steps [every 1000 iterations]')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "# This will plot the training loss over time.\n",
    "def plot_losses(list_of_losses, model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, losses in enumerate(list_of_losses):\n",
    "        plt.plot(range(len(losses)), losses, label=f'Run {i+1}')\n",
    "    plt.title(f'Training Loss over Time for {model_name}')\n",
    "    plt.xlabel('Evaluation Steps [every 1000 iterations]')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Download the Word2Vec file (Replaces !wget) ---\n",
    "# Check if the file already exists to avoid re-downloading\n",
    "url = \"https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt\"\n",
    "filename = \"googlenews.word2vec.300d.txt\"\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(\"Download complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "else:\n",
    "    print(f\"File '{filename}' already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Read and Process the Word Vectors (Replaces file access from Google Drive) ---\n",
    "# On your local machine, the file is in the current working directory,\n",
    "# so you can open it directly by its name.\n",
    "\n",
    "word2vec_data = {}\n",
    "lines_to_print = 4\n",
    "\n",
    "print(f\"\\nProcessing file and printing first {lines_to_print} lines:\")\n",
    "try:\n",
    "    with open(filename, encoding=\"utf-8\") as input_file:\n",
    "      i = 0\n",
    "      for line in input_file:\n",
    "        # Printing the first 4 lines as requested\n",
    "        if i < lines_to_print:\n",
    "          print(f\"Line {i+1}: {line.strip()}\")\n",
    "          # To check the vector length (skip first word/token)\n",
    "          print(f\"Vector Length: {len(line.split()[1:])}\\n\")\n",
    "        \n",
    "        # Converting to a dictionary format\n",
    "        line_parts = line.split()\n",
    "        if line_parts: # Ensure the line is not empty\n",
    "          word = line_parts[0]\n",
    "          # Convert the rest of the parts (the vector) to numpy float32\n",
    "          vector = [np.float32(x) for x in line_parts[1:]]\n",
    "          word2vec_data[word] = vector\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Optional: Stop after a few lines for testing to save memory/time\n",
    "        # if i > 1000: \n",
    "        #    break \n",
    "        \n",
    "    print(f\"\\nSuccessfully loaded {len(word2vec_data)} vectors into the dictionary.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nERROR: The file '{filename}' was not found. Please check if the download was successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while reading the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "word2vec_vocab = Vocabulary()\n",
    "vectors = []\n",
    "\n",
    "# taking the embedding dimension for the word2vec words\n",
    "embedding_dim = 300 #len(word2vec_data.values()[0]) # recheck\n",
    "print(f'our embed dim is {embedding_dim}')\n",
    "# creating a tensor with values distributed based on the normal distribution with mean 0 and variance 1\n",
    "# this is because for the unkown case we should be giving the embedding a\n",
    "# somewhat realistic distribution to an actual word\n",
    "# unk_vector = torch.randn(embedding_dim)\n",
    "unk_vector = np.random.randn(embedding_dim)\n",
    "vectors.append(unk_vector)\n",
    "word2vec_vocab.add_token('<unk>')\n",
    "# for padding, however, we do not want the embedding values to distort the\n",
    "# meaning of our sentence. Hence, it is better to apply zero-padding.\n",
    "pad_vector = np.zeros(embedding_dim)\n",
    "vectors.append(pad_vector)\n",
    "\n",
    "word2vec_vocab.add_token('<pad>')\n",
    "\n",
    "for token, embedding in word2vec_data.items():\n",
    "  #print(token)\n",
    "  #print(embedding)\n",
    "  word2vec_vocab.add_token(token)\n",
    "  vectors.append(np.array(embedding))\n",
    "  #break\n",
    "\n",
    "#word2vec_vocab.build()\n",
    "print(\"Vocabulary size:\", len(word2vec_vocab.w2i))\n",
    "print(embedding_dim)\n",
    "vectors = np.stack(vectors, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMCell(nn.Module):\n",
    "  \"\"\"Our own LSTM cell\"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, bias=True):\n",
    "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "    super(MyLSTMCell, self).__init__()\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.bias = bias\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    self.W_ii = torch.nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "    self.W_if = torch.nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "    self.W_ig = torch.nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "    self.W_io = torch.nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "    self.W_hi = torch.nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "    self.W_hf = torch.nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "    self.W_hg = torch.nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "    self.W_ho = torch.nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "    self.b_ii = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "    self.b_if = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "    self.b_ig = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "    self.b_io = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "    self.b_hi = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "    self.b_hf = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "    self.b_hg = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "    self.b_ho = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "    # end of my code\n",
    "\n",
    "    self.reset_parameters()\n",
    "\n",
    "  def reset_parameters(self):\n",
    "    \"\"\"This is PyTorch's default initialization method\"\"\"\n",
    "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "    for weight in self.parameters():\n",
    "      weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "  def forward(self, input_, hx, mask=None):\n",
    "    \"\"\"\n",
    "    input is (batch, input_size)\n",
    "    hx is ((batch, hidden_size), (batch, hidden_size))\n",
    "    \"\"\"\n",
    "    prev_h, prev_c = hx\n",
    "\n",
    "    # project input and prev state\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    i = torch.sigmoid(torch.matmul(input_, self.W_ii) + self.b_ii + torch.matmul(prev_h, self.W_hi) + self.b_hi)\n",
    "\n",
    "    #raise NotImplementedError(\"Implement this\")\n",
    "\n",
    "    # main LSTM computation\n",
    "\n",
    "    # i = ...\n",
    "    f = torch.sigmoid(torch.matmul(input_, self.W_if) + self.b_if + torch.matmul(prev_h, self.W_hf) + self.b_hf)\n",
    "    g = torch.tanh(torch.matmul(input_, self.W_ig) + self.b_ig + torch.matmul(prev_h, self.W_hg) + self.b_hg)\n",
    "    o = torch.sigmoid(torch.matmul(input_, self.W_io) + self.b_io + torch.matmul(prev_h, self.W_ho) + self.b_ho)\n",
    "    c = f * prev_c + i * g\n",
    "    h = o * torch.tanh(c)\n",
    "\n",
    "    return h, c\n",
    "\n",
    "  def __repr__(self):\n",
    "    return \"{}({:d}, {:d})\".format(\n",
    "        self.__class__.__name__, self.input_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "  \"\"\"Encodes sentence with an LSTM and projects final hidden state\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
    "    super(LSTMClassifier, self).__init__()\n",
    "    self.vocab = vocab\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "    self.rnn = MyLSTMCell(embedding_dim, hidden_dim)\n",
    "\n",
    "    self.output_layer = nn.Sequential(\n",
    "        nn.Dropout(p=0.5),  # explained later\n",
    "        nn.Linear(hidden_dim, output_dim)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    B = x.size(0)  # batch size (this is 1 for now, i.e. 1 single example)\n",
    "    T = x.size(1)  # timesteps (the number of words in the sentence)\n",
    "\n",
    "    input_ = self.embed(x)\n",
    "\n",
    "    # here we create initial hidden states containing zeros\n",
    "    # we use a trick here so that, if input is on the GPU, then so are hx and cx\n",
    "    hx = input_.new_zeros(B, self.rnn.hidden_size)\n",
    "    cx = input_.new_zeros(B, self.rnn.hidden_size)\n",
    "\n",
    "    # process input sentences one word/timestep at a time\n",
    "    # input is batch-major (i.e., batch size is the first dimension)\n",
    "    # so the first word(s) is (are) input_[:, 0]\n",
    "    outputs = []\n",
    "    for i in range(T):\n",
    "      hx, cx = self.rnn(input_[:, i], (hx, cx))\n",
    "      outputs.append(hx)\n",
    "\n",
    "    # if we have a single example, our final LSTM state is the last hx\n",
    "    if B == 1:\n",
    "      final = hx\n",
    "    else:\n",
    "      #\n",
    "      # This part is explained in next section, ignore this else-block for now.\n",
    "      #\n",
    "      # We processed sentences with different lengths, so some of the sentences\n",
    "      # had already finished and we have been adding padding inputs to hx.\n",
    "      # We select the final state based on the length of each sentence.\n",
    "\n",
    "      # two lines below not needed if using LSTM from pytorch\n",
    "      outputs = torch.stack(outputs, dim=0)           # [T, B, D]\n",
    "      outputs = outputs.transpose(0, 1).contiguous()  # [B, T, D]\n",
    "\n",
    "      # to be super-sure we're not accidentally indexing the wrong state\n",
    "      # we zero out positions that are invalid\n",
    "      pad_positions = (x == 1).unsqueeze(-1)\n",
    "\n",
    "      outputs = outputs.contiguous()\n",
    "      outputs = outputs.masked_fill_(pad_positions, 0.)\n",
    "\n",
    "      mask = (x != 1)  # true for valid positions [B, T]\n",
    "      lengths = mask.sum(dim=1)                 # [B, 1]\n",
    "\n",
    "      indexes = (lengths - 1) + torch.arange(B, device=x.device, dtype=x.dtype) * T\n",
    "      final = outputs.view(-1, self.hidden_dim)[indexes]  # [B, D]\n",
    "\n",
    "    # we use the last hidden state to classify the sentence\n",
    "    logits = self.output_layer(final)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(data, batch_size=25, shuffle=True):\n",
    "  \"\"\"Return minibatches, optional shuffling\"\"\"\n",
    "\n",
    "  if shuffle:\n",
    "    print(\"Shuffling training data\")\n",
    "    random.shuffle(data)  # shuffle training data each epoch\n",
    "\n",
    "  batch = []\n",
    "\n",
    "  # yield minibatches\n",
    "  for example in data:\n",
    "    batch.append(example)\n",
    "\n",
    "    if len(batch) == batch_size:\n",
    "      yield batch\n",
    "      batch = []\n",
    "\n",
    "  # in case there is something left\n",
    "  if len(batch) > 0:\n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(tokens, length, pad_value=1):\n",
    "  \"\"\"add padding 1s to a sequence to that it has the desired length\"\"\"\n",
    "  return tokens + [pad_value] * (length - len(tokens))\n",
    "\n",
    "# example\n",
    "tokens = [2, 3, 4]\n",
    "pad(tokens, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_minibatch(mb, vocab):\n",
    "  \"\"\"\n",
    "  Minibatch is a list of examples.\n",
    "  This function converts words to IDs and returns\n",
    "  torch tensors to be used as input/targets.\n",
    "  \"\"\"\n",
    "  batch_size = len(mb)\n",
    "  maxlen = max([len(ex.tokens) for ex in mb])\n",
    "\n",
    "  # vocab returns 0 if the word is not there\n",
    "  x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen) for ex in mb]\n",
    "\n",
    "  x = torch.LongTensor(x)\n",
    "  x = x.to(device)\n",
    "\n",
    "  y = [ex.label for ex in mb]\n",
    "  y = torch.LongTensor(y)\n",
    "  y = y.to(device)\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data,\n",
    "             batch_fn=get_minibatch, prep_fn=prepare_minibatch,\n",
    "             batch_size=16):\n",
    "  \"\"\"Accuracy of a model on given data set (using mini-batches)\"\"\"\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  model.eval()  # disable dropout\n",
    "\n",
    "  for mb in batch_fn(data, batch_size=batch_size, shuffle=False):\n",
    "    x, targets = prep_fn(mb, model.vocab)\n",
    "    with torch.no_grad():\n",
    "      logits = model(x)\n",
    "\n",
    "    predictions = logits.argmax(dim=-1).view(-1)\n",
    "\n",
    "    # add the number of correct predictions to the total correct\n",
    "    correct += (predictions == targets.view(-1)).sum().item()\n",
    "    total += targets.size(0)\n",
    "\n",
    "  return correct, total, correct / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here we print each parameter name, shape, and if it is trainable.\n",
    "def print_parameters(model):\n",
    "  total = 0\n",
    "  for name, p in model.named_parameters():\n",
    "    total += np.prod(p.shape)\n",
    "    print(\"{:24s} {:12s} requires_grad={}\".format(name, str(list(p.shape)), p.requires_grad))\n",
    "  print(\"\\nTotal number of parameters: {}\\n\".format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMClassifier(\n",
    "    len(word2vec_vocab.w2i), 300, 168, len(t2i), word2vec_vocab)\n",
    "\n",
    "# copy pre-trained vectors into embeddings table\n",
    "with torch.no_grad():\n",
    "  lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "  lstm_model.embed.weight.requires_grad = False # no fine-tuning yet\n",
    "\n",
    "print(lstm_model)\n",
    "print_parameters(lstm_model)\n",
    "\n",
    "lstm_model = lstm_model.to(device)\n",
    "\n",
    "optimizer_class = optim.Adam\n",
    "optimizer_params = {\"lr\": 3e-5}\n",
    "batch_size = 25\n",
    "json_lstm_minibatch = train_model_w_seed(lstm_model, optimizer_class, optimizer_params,\n",
    "                       train_data, dev_data, test_data,\n",
    "                       num_iterations=30000,\n",
    "                       print_every=1000,\n",
    "                       eval_every=1000,\n",
    "                       batch_size=batch_size,\n",
    "                       batch_fn=get_minibatch,\n",
    "                       prep_fn=prepare_minibatch,\n",
    "                       eval_fn=evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"lstm_model_minibatch.json\"\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(json_lstm_minibatch, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMClassifier(\n",
    "    len(word2vec_vocab.w2i), 300, 168, len(t2i), word2vec_vocab)\n",
    "\n",
    "# Now fine-tune your embeddings together with the model\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError(\"Implement this.\")\n",
    "# copy pre-trained word vectors into embeddings table\n",
    "with torch.no_grad():\n",
    "  lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "  lstm_model.embed.weight.requires_grad = True # <- this is where the fine-tuning takes place\n",
    "# end of my code\n",
    "\n",
    "print(lstm_model)\n",
    "print_parameters(lstm_model)\n",
    "\n",
    "lstm_model = lstm_model.to(device)\n",
    "json_lstm_minibatch_fine_tuned = train_model_w_seed(lstm_model, optimizer_class, optimizer_params,\n",
    "                       train_data, dev_data, test_data,\n",
    "                       num_iterations=30000,\n",
    "                       print_every=1000,\n",
    "                       eval_every=1000,\n",
    "                       batch_size=batch_size,\n",
    "                       batch_fn=get_minibatch,\n",
    "                       prep_fn=prepare_minibatch,\n",
    "                       eval_fn=evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"lstm_model_minibatch_fine_tuned.json\"\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(json_lstm_minibatch_fine_tuned, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
