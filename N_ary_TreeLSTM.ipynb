{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "filename = \"trainDevTestTrees_PTB.zip\"\n",
    "# --- Unzipping the file ---\n",
    "print(\"Extracting files...\")\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    # Extracts all contents into the current working directory\n",
    "    zip_ref.extractall(os.getcwd()) \n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function reads in a textfile and fixes an issue with \"\\\\\"\n",
    "def filereader(path):\n",
    "  with open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "      yield line.strip().replace(\"\\\\\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Tree\n",
    "from nltk.treeprettyprinter import TreePrettyPrinter\n",
    "\n",
    "s = next(filereader(\"trees/dev.txt\"))\n",
    "print(s)\n",
    "# We can use NLTK to better visualise the tree structure of the sentence\n",
    "from nltk import Tree\n",
    "from nltk.treeprettyprinter import TreePrettyPrinter\n",
    "tree = Tree.fromstring(s)\n",
    "print(TreePrettyPrinter(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first make a function that extracts the tokens (the leaves).\n",
    "\n",
    "def tokens_from_treestring(s):\n",
    "  \"\"\"extract the tokens from a sentiment tree\"\"\"\n",
    "  return re.sub(r\"\\([0-9] |\\)\", \"\", s).split()\n",
    "\n",
    "# let's try it on our example tree\n",
    "tokens = tokens_from_treestring(s)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also need the following function, but you can ignore this for now.\n",
    "# It is explained later on.\n",
    "\n",
    "SHIFT = 0\n",
    "REDUCE = 1\n",
    "\n",
    "\n",
    "def transitions_from_treestring(s):\n",
    "  s = re.sub(\"\\([0-5] ([^)]+)\\)\", \"0\", s)\n",
    "  s = re.sub(\"\\)\", \" )\", s)\n",
    "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
    "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
    "  s = re.sub(\"\\)\", \"1\", s)\n",
    "  return list(map(int, s.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's first see how large our data sets are.\n",
    "for path in (\"trees/train.txt\", \"trees/dev.txt\", \"trees/test.txt\"):\n",
    "  print(\"{:16s} {:4d}\".format(path, sum(1 for _ in filereader(path))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from nltk import Tree\n",
    "\n",
    "# A simple way to define a class is using namedtuple.\n",
    "Example = namedtuple(\"Example\", [\"tokens\", \"tree\", \"label\", \"transitions\"])\n",
    "\n",
    "\n",
    "def examplereader(path, lower=False):\n",
    "  \"\"\"Returns all examples in a file one by one.\"\"\"\n",
    "  for line in filereader(path):\n",
    "    line = line.lower() if lower else line\n",
    "    tokens = tokens_from_treestring(line)\n",
    "    tree = Tree.fromstring(line)  # use NLTK's Tree\n",
    "    label = int(line[1])\n",
    "    trans = transitions_from_treestring(line)\n",
    "    yield Example(tokens=tokens, tree=tree, label=label, transitions=trans)\n",
    "\n",
    "\n",
    "# Let's load the data into memory.\n",
    "LOWER = False  # we will keep the original casing\n",
    "train_data = list(examplereader(\"trees/train.txt\", lower=LOWER))\n",
    "dev_data = list(examplereader(\"trees/dev.txt\", lower=LOWER))\n",
    "test_data = list(examplereader(\"trees/test.txt\", lower=LOWER))\n",
    "\n",
    "print(\"train\", len(train_data))\n",
    "print(\"dev\", len(dev_data))\n",
    "print(\"test\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first define a class that can map a word to an ID (w2i)\n",
    "# and back (i2w).\n",
    "\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "  \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
    "  def __repr__(self):\n",
    "    return '%s(%r)' % (self.__class__.__name__,\n",
    "                      OrderedDict(self))\n",
    "  def __reduce__(self):\n",
    "    return self.__class__, (OrderedDict(self),)\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "  \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self.freqs = OrderedCounter()\n",
    "    self.w2i = {}\n",
    "    self.i2w = []\n",
    "\n",
    "  def count_token(self, t):\n",
    "    self.freqs[t] += 1\n",
    "\n",
    "  def add_token(self, t):\n",
    "    self.w2i[t] = len(self.w2i)\n",
    "    self.i2w.append(t)\n",
    "\n",
    "  def build(self, min_freq=0):\n",
    "    '''\n",
    "    min_freq: minimum number of occurrences for a word to be included\n",
    "              in the vocabulary\n",
    "    '''\n",
    "    self.add_token(\"<unk>\")  # reserve 0 for <unk> (unknown words)\n",
    "    self.add_token(\"<pad>\")  # reserve 1 for <pad> (discussed later)\n",
    "\n",
    "    tok_freq = list(self.freqs.items())\n",
    "    tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "    for tok, freq in tok_freq:\n",
    "      if freq >= min_freq:\n",
    "        self.add_token(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This process should be deterministic and should have the same result\n",
    "# if run multiple times on the same data set.\n",
    "\n",
    "v = Vocabulary()\n",
    "for data_set in (train_data,):\n",
    "  for ex in data_set:\n",
    "    for token in ex.tokens:\n",
    "      v.count_token(token)\n",
    "\n",
    "v.build()\n",
    "print(\"Vocabulary size:\", len(v.w2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's map the sentiment labels 0-4 to a more readable form\n",
    "i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
    "print(i2t)\n",
    "print(i2t[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's also create the opposite mapping.\n",
    "# We won't use a Vocabulary for this (although we could), since the labels\n",
    "# are already numeric.\n",
    "t2i = OrderedDict({p : i for p, i in zip(i2t, range(len(i2t)))})\n",
    "print(t2i)\n",
    "print(t2i['very positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_example(example, vocab):\n",
    "  \"\"\"\n",
    "  Map tokens to their IDs for a single example\n",
    "  \"\"\"\n",
    "\n",
    "  # vocab returns 0 if the word is not there (i2w[0] = <unk>)\n",
    "  x = [vocab.w2i.get(t, 0) for t in example.tokens]\n",
    "\n",
    "  x = torch.LongTensor([x])\n",
    "  x = x.to(device)\n",
    "\n",
    "  y = torch.LongTensor([example.label])\n",
    "  y = y.to(device)\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
    "  \"\"\"Accuracy of a model on given data set.\"\"\"\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  model.eval()  # disable dropout (explained later)\n",
    "\n",
    "  for example in data:\n",
    "\n",
    "    # convert the example input and label to PyTorch tensors\n",
    "    x, target = prep_fn(example, model.vocab)\n",
    "\n",
    "    # forward pass without backpropagation (no_grad)\n",
    "    # get the output from the neural network for input x\n",
    "    with torch.no_grad():\n",
    "      logits = model(x)\n",
    "\n",
    "    # get the prediction\n",
    "    prediction = logits.argmax(dim=-1)\n",
    "\n",
    "    # add the number of correct predictions to the total correct\n",
    "    correct += (prediction == target).sum().item()\n",
    "    total += 1\n",
    "\n",
    "  return correct, total, correct / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(data, shuffle=True, **kwargs):\n",
    "  \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
    "  if shuffle:\n",
    "    print(\"Shuffling training data\")\n",
    "    random.shuffle(data)  # shuffle training data each epoch\n",
    "  for example in data:\n",
    "    yield example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, optimizer, num_iterations=10000,\n",
    "                print_every=1000, eval_every=1000,\n",
    "                batch_fn=get_examples,\n",
    "                prep_fn=prepare_example,\n",
    "                eval_fn=simple_evaluate,\n",
    "                batch_size=1, eval_batch_size=None, seed=17):\n",
    "  \"\"\"Train a model.\"\"\"\n",
    "  iter_i = 0\n",
    "  train_loss = 0.\n",
    "  print_num = 0\n",
    "  start = time.time()\n",
    "  criterion = nn.CrossEntropyLoss() # loss function\n",
    "  best_eval = 0.\n",
    "  best_iter = 0\n",
    "\n",
    "  # store train loss and validation accuracy during training\n",
    "  # so we can plot them afterwards\n",
    "  losses = []\n",
    "  accuracies = []\n",
    "\n",
    "  if eval_batch_size is None:\n",
    "    eval_batch_size = batch_size\n",
    "\n",
    "  while True:  # when we run out of examples, shuffle and continue\n",
    "    for batch in tqdm(batch_fn(train_data, batch_size=batch_size)): \n",
    "\n",
    "      # forward pass\n",
    "      model.train()\n",
    "      x, targets = prep_fn(batch, model.vocab)\n",
    "      logits = model(x)\n",
    "\n",
    "      B = targets.size(0)  # later we will use B examples per update\n",
    "\n",
    "      # compute cross-entropy loss (our criterion)\n",
    "      # note that the cross entropy loss function computes the softmax for us\n",
    "      loss = criterion(logits.view([B, -1]), targets.view(-1))\n",
    "      train_loss += loss.item()\n",
    "\n",
    "      # backward pass (tip: check the Introduction to PyTorch notebook)\n",
    "\n",
    "      # ========== MY CODE ==============\n",
    "      # erase previous gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # compute gradients\n",
    "      loss.backward()\n",
    "\n",
    "      # update weights - take a small step in the opposite dir of the gradient\n",
    "      optimizer.step()\n",
    "\n",
    "      print_num += 1\n",
    "      iter_i += 1\n",
    "\n",
    "      # print info\n",
    "      if iter_i % print_every == 0:\n",
    "        print(\"Iter %r: loss=%.4f, time=%.2fs\" %\n",
    "              (iter_i, train_loss, time.time()-start))\n",
    "        losses.append(train_loss)\n",
    "        print_num = 0\n",
    "        train_loss = 0.\n",
    "\n",
    "      # evaluate\n",
    "      if iter_i % eval_every == 0:\n",
    "        _, _, accuracy = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                                 batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"iter %r: dev acc=%.4f\" % (iter_i, accuracy))\n",
    "\n",
    "        # save best model parameters\n",
    "        if accuracy > best_eval:\n",
    "          print(\"new highscore\")\n",
    "          best_eval = accuracy\n",
    "          best_iter = iter_i\n",
    "          path = \"{}_{}.pt\".format(model.__class__.__name__, seed)\n",
    "          ckpt = {\n",
    "              \"state_dict\": model.state_dict(),\n",
    "              \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "              \"best_eval\": best_eval,\n",
    "              \"best_iter\": best_iter\n",
    "          }\n",
    "          torch.save(ckpt, path)\n",
    "\n",
    "      # done training\n",
    "      if iter_i == num_iterations:\n",
    "        print(\"Done training\")\n",
    "\n",
    "        # evaluate on train, dev, and test with best model\n",
    "        print(\"Loading best model\")\n",
    "        path = \"{}_{}.pt\".format(model.__class__.__name__, seed)\n",
    "        ckpt = torch.load(path)\n",
    "        model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "        _, _, train_acc = eval_fn(\n",
    "            model, train_data, batch_size=eval_batch_size,\n",
    "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "        _, _, dev_acc = eval_fn(\n",
    "            model, dev_data, batch_size=eval_batch_size,\n",
    "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "        _, _, test_acc = eval_fn(\n",
    "            model, test_data, batch_size=eval_batch_size,\n",
    "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "\n",
    "        print(\"best model iter {:d}: \"\n",
    "              \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "                  best_iter, train_acc, dev_acc, test_acc))\n",
    "\n",
    "        return losses, accuracies, best_iter, train_acc, dev_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import random\n",
    "\n",
    "def train_model_w_seed(model, optimizer_class, optimizer_params,\n",
    "                       train_data, dev_data, test_data,\n",
    "                       num_iterations=10000,\n",
    "                       print_every=1000,\n",
    "                       eval_every=1000,\n",
    "                       batch_fn=get_examples,\n",
    "                       prep_fn=prepare_example,\n",
    "                       eval_fn=simple_evaluate,\n",
    "                       batch_size=1,\n",
    "                       eval_batch_size=None):\n",
    "\n",
    "    test_accs = []\n",
    "    best_iters = []\n",
    "    train_accs = []\n",
    "    dev_accs = []\n",
    "\n",
    "    list_of_accuracies = []\n",
    "    list_of_losses = []\n",
    "\n",
    "    # create 3 copies of the model\n",
    "    models = [copy.deepcopy(model) for _ in range(3)]\n",
    "    seeds = [17, 42, 2025]\n",
    "\n",
    "    for seed, model_copy in zip(seeds, models):\n",
    "        # Set seeds for reproducibility\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        # Create a fresh optimizer for each model\n",
    "        # optimizer = optimizer_class(model_copy.parameters(), **optimizer_params)\n",
    "        # Changed this to optimize only parameters that require gradients\n",
    "        optimizer = optimizer_class(\n",
    "            filter(lambda p: p.requires_grad, model_copy.parameters()),\n",
    "            **optimizer_params\n",
    "        )\n",
    "\n",
    "        # Call your inner training function\n",
    "        # Make sure your train_model function returns:\n",
    "        # losses, accuracies, best_iter, train_acc, dev_acc, test_acc\n",
    "        losses, accuracies, best_iter, train_acc, dev_acc, test_acc = train_model(\n",
    "            model_copy, optimizer,\n",
    "            num_iterations=num_iterations,\n",
    "            print_every=print_every,\n",
    "            eval_every=eval_every,\n",
    "            batch_fn=batch_fn,\n",
    "            prep_fn=prep_fn,\n",
    "            eval_fn=eval_fn,\n",
    "            batch_size=batch_size,\n",
    "            eval_batch_size=eval_batch_size,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        list_of_accuracies.append(accuracies)\n",
    "        list_of_losses.append(losses)\n",
    "        best_iters.append(best_iter)\n",
    "        train_accs.append(train_acc)\n",
    "        dev_accs.append(dev_acc)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "    # Compute mean and std\n",
    "    mean_train_acc = np.mean(train_accs)\n",
    "    mean_dev_acc = np.mean(dev_accs)\n",
    "    mean_test_acc = np.mean(test_accs)\n",
    "    mean_best_iter = np.mean(best_iters)\n",
    "\n",
    "    std_train_acc = np.std(train_accs)\n",
    "    std_dev_acc = np.std(dev_accs)\n",
    "    std_test_acc = np.std(test_accs)\n",
    "    std_best_iter = np.std(best_iters)\n",
    "\n",
    "    # Create JSON\n",
    "    json_data = {\n",
    "        'train_accs_mean': mean_train_acc,\n",
    "        'dev_accs_mean': mean_dev_acc,\n",
    "        'test_accs_mean': mean_test_acc,\n",
    "\n",
    "        'train_accs_std': std_train_acc,\n",
    "        'dev_accs_std': std_dev_acc,\n",
    "        'test_accs_std': std_test_acc,\n",
    "\n",
    "        'train_accs': train_accs,\n",
    "        'dev_accs': dev_accs,\n",
    "        'test_accs': test_accs,\n",
    "        'best_iters': best_iters,\n",
    "\n",
    "        'list_of_losses': list_of_losses,\n",
    "        'list_of_accuracies': list_of_accuracies\n",
    "    }\n",
    "\n",
    "    # Save JSON file once\n",
    "    filename = f\"{model.__class__.__name__}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the validation accuracies over time\n",
    "def plot_accuracies(list_of_accuracies, model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, accuracies in enumerate(list_of_accuracies):\n",
    "        plt.plot(range(len(accuracies)), accuracies, label=f'Run {i+1}')\n",
    "    plt.title(f'Validation Accuracies over Time for {model_name}')\n",
    "    plt.xlabel('Evaluation Steps [every 1000 iterations]')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "# This will plot the training loss over time.\n",
    "def plot_losses(list_of_losses, model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, losses in enumerate(list_of_losses):\n",
    "        plt.plot(range(len(losses)), losses, label=f'Run {i+1}')\n",
    "    plt.title(f'Training Loss over Time for {model_name}')\n",
    "    plt.xlabel('Evaluation Steps [every 1000 iterations]')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Download the Word2Vec file (Replaces !wget) ---\n",
    "# Check if the file already exists to avoid re-downloading\n",
    "url = \"https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt\"\n",
    "filename = \"googlenews.word2vec.300d.txt\"\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(\"Download complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "else:\n",
    "    print(f\"File '{filename}' already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Read and Process the Word Vectors (Replaces file access from Google Drive) ---\n",
    "# On your local machine, the file is in the current working directory,\n",
    "# so you can open it directly by its name.\n",
    "\n",
    "word2vec_data = {}\n",
    "lines_to_print = 4\n",
    "\n",
    "print(f\"\\nProcessing file and printing first {lines_to_print} lines:\")\n",
    "try:\n",
    "    with open(filename, encoding=\"utf-8\") as input_file:\n",
    "      i = 0\n",
    "      for line in input_file:\n",
    "        # Printing the first 4 lines as requested\n",
    "        if i < lines_to_print:\n",
    "          print(f\"Line {i+1}: {line.strip()}\")\n",
    "          # To check the vector length (skip first word/token)\n",
    "          print(f\"Vector Length: {len(line.split()[1:])}\\n\")\n",
    "        \n",
    "        # Converting to a dictionary format\n",
    "        line_parts = line.split()\n",
    "        if line_parts: # Ensure the line is not empty\n",
    "          word = line_parts[0]\n",
    "          # Convert the rest of the parts (the vector) to numpy float32\n",
    "          vector = [np.float32(x) for x in line_parts[1:]]\n",
    "          word2vec_data[word] = vector\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Optional: Stop after a few lines for testing to save memory/time\n",
    "        # if i > 1000: \n",
    "        #    break \n",
    "        \n",
    "    print(f\"\\nSuccessfully loaded {len(word2vec_data)} vectors into the dictionary.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nERROR: The file '{filename}' was not found. Please check if the download was successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while reading the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "word2vec_vocab = Vocabulary()\n",
    "vectors = []\n",
    "\n",
    "# taking the embedding dimension for the word2vec words\n",
    "embedding_dim = 300 #len(word2vec_data.values()[0]) # recheck\n",
    "print(f'our embed dim is {embedding_dim}')\n",
    "# creating a tensor with values distributed based on the normal distribution with mean 0 and variance 1\n",
    "# this is because for the unkown case we should be giving the embedding a\n",
    "# somewhat realistic distribution to an actual word\n",
    "# unk_vector = torch.randn(embedding_dim)\n",
    "unk_vector = np.random.randn(embedding_dim)\n",
    "vectors.append(unk_vector)\n",
    "word2vec_vocab.add_token('<unk>')\n",
    "# for padding, however, we do not want the embedding values to distort the\n",
    "# meaning of our sentence. Hence, it is better to apply zero-padding.\n",
    "pad_vector = np.zeros(embedding_dim)\n",
    "vectors.append(pad_vector)\n",
    "\n",
    "word2vec_vocab.add_token('<pad>')\n",
    "\n",
    "for token, embedding in word2vec_data.items():\n",
    "  #print(token)\n",
    "  #print(embedding)\n",
    "  word2vec_vocab.add_token(token)\n",
    "  vectors.append(np.array(embedding))\n",
    "  #break\n",
    "\n",
    "#word2vec_vocab.build()\n",
    "print(\"Vocabulary size:\", len(word2vec_vocab.w2i))\n",
    "print(embedding_dim)\n",
    "vectors = np.stack(vectors, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(data, batch_size=25, shuffle=True):\n",
    "  \"\"\"Return minibatches, optional shuffling\"\"\"\n",
    "\n",
    "  if shuffle:\n",
    "    print(\"Shuffling training data\")\n",
    "    random.shuffle(data)  # shuffle training data each epoch\n",
    "\n",
    "  batch = []\n",
    "\n",
    "  # yield minibatches\n",
    "  for example in data:\n",
    "    batch.append(example)\n",
    "\n",
    "    if len(batch) == batch_size:\n",
    "      yield batch\n",
    "      batch = []\n",
    "\n",
    "  # in case there is something left\n",
    "  if len(batch) > 0:\n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(tokens, length, pad_value=1):\n",
    "  \"\"\"add padding 1s to a sequence to that it has the desired length\"\"\"\n",
    "  return tokens + [pad_value] * (length - len(tokens))\n",
    "\n",
    "# example\n",
    "tokens = [2, 3, 4]\n",
    "pad(tokens, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_minibatch(mb, vocab):\n",
    "  \"\"\"\n",
    "  Minibatch is a list of examples.\n",
    "  This function converts words to IDs and returns\n",
    "  torch tensors to be used as input/targets.\n",
    "  \"\"\"\n",
    "  batch_size = len(mb)\n",
    "  maxlen = max([len(ex.tokens) for ex in mb])\n",
    "\n",
    "  # vocab returns 0 if the word is not there\n",
    "  x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen) for ex in mb]\n",
    "\n",
    "  x = torch.LongTensor(x)\n",
    "  x = x.to(device)\n",
    "\n",
    "  y = [ex.label for ex in mb]\n",
    "  y = torch.LongTensor(y)\n",
    "  y = y.to(device)\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data,\n",
    "             batch_fn=get_minibatch, prep_fn=prepare_minibatch,\n",
    "             batch_size=16):\n",
    "  \"\"\"Accuracy of a model on given data set (using mini-batches)\"\"\"\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  model.eval()  # disable dropout\n",
    "\n",
    "  for mb in batch_fn(data, batch_size=batch_size, shuffle=False):\n",
    "    x, targets = prep_fn(mb, model.vocab)\n",
    "    with torch.no_grad():\n",
    "      logits = model(x)\n",
    "\n",
    "    predictions = logits.argmax(dim=-1).view(-1)\n",
    "\n",
    "    # add the number of correct predictions to the total correct\n",
    "    correct += (predictions == targets.view(-1)).sum().item()\n",
    "    total += targets.size(0)\n",
    "\n",
    "  return correct, total, correct / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here we print each parameter name, shape, and if it is trainable.\n",
    "def print_parameters(model):\n",
    "  total = 0\n",
    "  for name, p in model.named_parameters():\n",
    "    total += np.prod(p.shape)\n",
    "    print(\"{:24s} {:12s} requires_grad={}\".format(name, str(list(p.shape)), p.requires_grad))\n",
    "  print(\"\\nTotal number of parameters: {}\\n\".format(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeLSTMCell(nn.Module):\n",
    "  \"\"\"A Binary Tree LSTM cell\"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, bias=True):\n",
    "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "    super(TreeLSTMCell, self).__init__()\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.bias = bias\n",
    "\n",
    "    self.reduce_layer = nn.Linear(2 * hidden_size, 5 * hidden_size)\n",
    "    self.dropout_layer = nn.Dropout(p=0.25)\n",
    "\n",
    "    self.reset_parameters()\n",
    "\n",
    "  def reset_parameters(self):\n",
    "    \"\"\"This is PyTorch's default initialization method\"\"\"\n",
    "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "    for weight in self.parameters():\n",
    "      weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "  def forward(self, hx_l, hx_r, mask=None):\n",
    "    \"\"\"\n",
    "    hx_l is ((batch, hidden_size), (batch, hidden_size))\n",
    "    hx_r is ((batch, hidden_size), (batch, hidden_size))\n",
    "    \"\"\"\n",
    "    prev_h_l, prev_c_l = hx_l  # left child\n",
    "    prev_h_r, prev_c_r = hx_r  # right child\n",
    "\n",
    "    B = prev_h_l.size(0)\n",
    "\n",
    "    # we concatenate the left and right children\n",
    "    # you can also project from them separately and then sum\n",
    "    children = torch.cat([prev_h_l, prev_h_r], dim=1)\n",
    "\n",
    "    # project the combined children into a 5D tensor for i,fl,fr,g,o\n",
    "    # this is done for speed, and you could also do it separately\n",
    "    proj = self.reduce_layer(children)  # shape: B x 5D\n",
    "\n",
    "    proj = self.dropout_layer(proj)\n",
    "\n",
    "    # each shape: B x D\n",
    "    i, f_l, f_r, g, o = torch.chunk(proj, 5, dim=-1)\n",
    "\n",
    "    # main Tree LSTM computation\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # You only need to complete the commented lines below.\n",
    "    # raise NotImplementedError(\"Implement this.\")\n",
    "\n",
    "    # The shape of each of these is [batch_size, hidden_size]\n",
    "\n",
    "    i = torch.sigmoid(i)\n",
    "    f_l = torch.sigmoid(f_l)\n",
    "    f_r = torch.sigmoid(f_r)\n",
    "    g = torch.tanh(g)\n",
    "\n",
    "    g = self.dropout_layer(g)\n",
    "    o = torch.sigmoid(o)\n",
    "\n",
    "    c = f_l * prev_c_l + f_r * prev_c_r + i * g\n",
    "    h = o * torch.tanh(c)\n",
    "\n",
    "    return h, c\n",
    "\n",
    "  def __repr__(self):\n",
    "    return \"{}({:d}, {:d})\".format(\n",
    "        self.__class__.__name__, self.input_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for batching and unbatching states\n",
    "# For speed we want to combine computations by batching, but\n",
    "# for processing logic we want to turn the output into lists again\n",
    "# to easily manipulate.\n",
    "\n",
    "def batch(states):\n",
    "  \"\"\"\n",
    "  Turns a list of states into a single tensor for fast processing.\n",
    "  This function also chunks (splits) each state into a (h, c) pair\"\"\"\n",
    "  return torch.cat(states, 0).chunk(2, 1)\n",
    "\n",
    "def unbatch(state):\n",
    "  \"\"\"\n",
    "  Turns a tensor back into a list of states.\n",
    "  First, (h, c) are merged into a single state.\n",
    "  Then the result is split into a list of sentences.\n",
    "  \"\"\"\n",
    "  return torch.split(torch.cat(state, 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeLSTM(nn.Module):\n",
    "  \"\"\"Encodes a sentence using a TreeLSTMCell\"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, bias=True):\n",
    "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "    super(TreeLSTM, self).__init__()\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.bias = bias\n",
    "    self.reduce = TreeLSTMCell(input_size, hidden_size)\n",
    "\n",
    "    # project word to initial c\n",
    "    self.proj_x = nn.Linear(input_size, hidden_size)\n",
    "    self.proj_x_gate = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "    self.buffers_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "  def forward(self, x, transitions):\n",
    "    \"\"\"\n",
    "    WARNING: assuming x is reversed!\n",
    "    :param x: word embeddings [B, T, E]\n",
    "    :param transitions: [2T-1, B]\n",
    "    :return: root states\n",
    "    \"\"\"\n",
    "\n",
    "    B = x.size(0)  # batch size\n",
    "    T = x.size(1)  # time\n",
    "\n",
    "    # compute an initial c and h for each word\n",
    "    # Note: this corresponds to input x in the Tai et al. Tree LSTM paper.\n",
    "    # We do not handle input x in the TreeLSTMCell itself.\n",
    "    buffers_c = self.proj_x(x)\n",
    "    buffers_h = buffers_c.tanh()\n",
    "    buffers_h_gate = self.proj_x_gate(x).sigmoid()\n",
    "    buffers_h = buffers_h_gate * buffers_h\n",
    "\n",
    "    # concatenate h and c for each word\n",
    "    buffers = torch.cat([buffers_h, buffers_c], dim=-1)\n",
    "\n",
    "    D = buffers.size(-1) // 2\n",
    "\n",
    "    # we turn buffers into a list of stacks (1 stack for each sentence)\n",
    "    # first we split buffers so that it is a list of sentences (length B)\n",
    "    # then we split each sentence to be a list of word vectors\n",
    "    buffers = buffers.split(1, dim=0)  # Bx[T, 2D]\n",
    "    buffers = [list(b.squeeze(0).split(1, dim=0)) for b in buffers]  # BxTx[2D]\n",
    "\n",
    "    # create B empty stacks\n",
    "    stacks = [[] for _ in buffers]\n",
    "\n",
    "    # t_batch holds 1 transition for each sentence\n",
    "    for t_batch in transitions:\n",
    "\n",
    "      child_l = []  # contains the left child for each sentence with reduce action\n",
    "      child_r = []  # contains the corresponding right child\n",
    "\n",
    "      # iterate over sentences in the batch\n",
    "      # each has a transition t, a buffer and a stack\n",
    "      for transition, buffer, stack in zip(t_batch, buffers, stacks):\n",
    "        if transition == SHIFT:\n",
    "          stack.append(buffer.pop())\n",
    "        elif transition == REDUCE:\n",
    "          assert len(stack) >= 2, \\\n",
    "            \"Stack too small! Should not happen with valid transition sequences\"\n",
    "          child_r.append(stack.pop())  # right child is on top\n",
    "          child_l.append(stack.pop())\n",
    "\n",
    "      # if there are sentences with reduce transition, perform them batched\n",
    "      if child_l:\n",
    "        reduced = iter(unbatch(self.reduce(batch(child_l), batch(child_r))))\n",
    "        for transition, stack in zip(t_batch, stacks):\n",
    "          if transition == REDUCE:\n",
    "            stack.append(next(reduced))\n",
    "\n",
    "    final = [stack.pop().chunk(2, -1)[0] for stack in stacks]\n",
    "    final = torch.cat(final, dim=0)  # tensor [B, D]\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeLSTMClassifier(nn.Module):\n",
    "  \"\"\"Encodes sentence with a TreeLSTM and projects final hidden state\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
    "    super(TreeLSTMClassifier, self).__init__()\n",
    "    self.vocab = vocab\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "    self.treelstm = TreeLSTM(embedding_dim, hidden_dim)\n",
    "    self.output_layer = nn.Sequential(\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(hidden_dim, output_dim, bias=True)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    # x is a pair here of words and transitions; we unpack it here.\n",
    "    # x is batch-major: [B, T], transitions is time major [2T-1, B]\n",
    "    x, transitions = x\n",
    "    emb = self.embed(x)\n",
    "\n",
    "    # we use the root/top state of the Tree LSTM to classify the sentence\n",
    "    root_states = self.treelstm(emb, transitions)\n",
    "\n",
    "    # we use the last hidden state to classify the sentence\n",
    "    logits = self.output_layer(root_states)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_treelstm_minibatch(mb, vocab):\n",
    "  \"\"\"\n",
    "  Returns sentences reversed (last word first)\n",
    "  Returns transitions together with the sentences.\n",
    "  \"\"\"\n",
    "  batch_size = len(mb)\n",
    "  maxlen = max([len(ex.tokens) for ex in mb])\n",
    "\n",
    "  # vocab returns 0 if the word is not there\n",
    "  # NOTE: reversed sequence!\n",
    "  x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen)[::-1] for ex in mb]\n",
    "\n",
    "  x = torch.LongTensor(x)\n",
    "  x = x.to(device)\n",
    "\n",
    "  y = [ex.label for ex in mb]\n",
    "  y = torch.LongTensor(y)\n",
    "  y = y.to(device)\n",
    "\n",
    "  maxlen_t = max([len(ex.transitions) for ex in mb])\n",
    "  transitions = [pad(ex.transitions, maxlen_t, pad_value=2) for ex in mb]\n",
    "  transitions = np.array(transitions)\n",
    "  transitions = transitions.T  # time-major\n",
    "\n",
    "  return (x, transitions), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the N-ary LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's train the Tree LSTM!\n",
    "\n",
    "tree_model = TreeLSTMClassifier(\n",
    "    len(word2vec_vocab.w2i), 300, 150, len(t2i), word2vec_vocab)\n",
    "\n",
    "with torch.no_grad():\n",
    "  tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "  tree_model.embed.weight.requires_grad = False\n",
    "\n",
    "def do_train(model):\n",
    "\n",
    "  print(model)\n",
    "  print_parameters(model)\n",
    "\n",
    "  model = model.to(device)\n",
    "  optimizer_class = optim.Adam\n",
    "  optimizer_params = {\"lr\": 3e-5}\n",
    "\n",
    "  return train_model_w_seed(\n",
    "      model, optimizer_class, optimizer_params,\n",
    "      train_data, dev_data, test_data,\n",
    "      num_iterations=30000,\n",
    "      print_every=1000, eval_every=1000,\n",
    "      prep_fn=prepare_treelstm_minibatch,\n",
    "      eval_fn=evaluate,\n",
    "      batch_fn=get_minibatch,\n",
    "      batch_size=25, eval_batch_size=25)\n",
    "\n",
    "n_ary_tree = do_train(tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(n_ary_tree['list_of_accuracies'], \"N-ary Tree LSTM\")\n",
    "plot_losses(n_ary_tree['list_of_losses'], \"N-ary Tree LSTM\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
